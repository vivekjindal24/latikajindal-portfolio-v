export interface CLO {
  id: string;
  description: string;
}

export interface CO {
  id: string;
  description: string;
}

export interface Topic {
  id: string;
  title: string;
  subtopics?: string[];
  clos: string[];
  cos: string[];
  content: {
    introduction: string;
    concept: string;
    technicalDepth: string;
    examples: string;
    practical: string;
    exam: string;
    takeaways: string;
  };
}

export interface Unit {
  id: string;
  title: string;
  topics: Topic[];
}

export const clos: CLO[] = [
  { id: "CLO01", description: "Have a broad understanding of database concepts and database management system software" },
  { id: "CLO02", description: "Have a high-level understanding of major DBMS components and their function" },
  { id: "CLO03", description: "Be able to model an application's data requirements using conceptual modeling tools like ER diagrams and design database schemas based on the conceptual model." },
  { id: "CLO04", description: "Be able to write SQL commands to create tables and indexes, insert/update/delete data, and query data in a relational DBMS." },
  { id: "CLO05", description: "Be able to program a data-intensive application using DBMS APIs." },
];

export const cos: CO[] = [
  { id: "CO01", description: "Identify the basic concepts and various data model used in database design ER modelling concepts and architecture use and design queries using SQL" },
  { id: "CO02", description: "Apply relational database theory and be able to describe relational algebra expression, tuple and domain relation expression from queries." },
  { id: "CO03", description: "Recognize and identify the use of normalization and functional dependency. indexing and hashing technique used in database design." },
  { id: "CO04", description: "Recognize/ Identify the purpose of query processing and optimization and also demonstrate the basic of query evaluation." },
  { id: "CO05", description: "Apply and relate the concept of transaction, concurrency control and recovery in database." },
  { id: "CO06", description: "Understanding of recovery system and be familiar with introduction to web database, distribute databases, data warehousing and mining." },
];

export const units: Unit[] = [
  {
    id: "unit-1",
    title: "UNIT I: Introduction & Architecture",
    topics: [
      {
        id: "intro-to-db",
        title: "Introduction to Database",
        subtopics: ["Hierarchical, Network and Relational database"],
        clos: ["CLO01"],
        cos: ["CO01"],
        content: {
          introduction: "Database systems form the backbone of modern information management, handling everything from banking transactions to social media interactions. A Database Management System (DBMS) is a collection of interrelated data and a set of programs to access that data, providing a convenient and efficient way to store and retrieve database information. The primary goal is to manage data in a way that minimizes redundancy and ensures data integrity while allowing multiple users concurrent access.",
          concept: "A database is a collection of related data organized to serve multiple applications efficiently. Data represents recordable facts with implicit meaning, such as names, addresses, and balances. A DBMS serves as an interface between the database and end users or application programs.\n\n### Data vs Information (Quick Comparison)\n\n| Data | Information |\n|---|---|\n| Raw facts and figures | Processed, organized, and meaningful output |\n| May be unorganized (numbers, symbols, text) | Interpretable and useful for decision-making |\n| Input to a system | Output of processing |\n| Example: 95, 88, 76 | Example: Average = 86.33, Grade = A |\n| By itself may not add value | Adds value and context |\n\n### Key Characteristics of the Database Approach\n\n1. Self-describing nature: Database contains not only data but also metadata (data about data) in the system catalog\n\n2. Insulation between programs and data: Program-data independence allows changing data structure without modifying programs\n\n3. Support for multiple views: Different users can have different perspectives of the same data\n\n4. Sharing of data and multiuser transaction processing: Multiple users can access database simultaneously while maintaining consistency\n\n![Advantages of DBMS](/images/unit1/Advantages%20of%20DBMS.png)\n\n### Advantages of Using a DBMS\n- Controlling redundancy: Eliminates duplicate data storage\n- Restricting unauthorized access: Security and authorization mechanisms\n- Providing persistent storage: Data outlives programs that create it\n- Providing backup and recovery: Protection against hardware/software failures\n- Enforcing integrity constraints: Rules to maintain data accuracy\n- Permitting inference and actions using rules: Database triggers and stored procedures\n\n![Drawbacks of File Systems](/images/unit1/Drawbacks%20of%20using%20file%20systems%20to%20store%20data.png)\n\n### Disadvantages of File-Based Systems\nThe file-based approach suffers from data redundancy, inconsistency, difficulty in accessing data, data isolation, integrity problems, atomicity issues, concurrent access anomalies, and security problems — all of which are addressed by a DBMS.",
          technicalDepth: "DATABASE MODELS EVOLUTION:\n\n![Database Models Overview](/images/unit1/er%20data%20model.png)\n\n1. HIERARCHICAL MODEL (1960s-1970s):\n\n![Hierarchical Data Model](/images/unit1/hirerarchical%20data%20model.png)\n\nStructure: Tree-like hierarchy with parent-child relationships\nCharacteristics:\n  • One-to-many (1:N) relationships only\n  • Single root for each tree\n  • Each child has exactly one parent\n  • Navigation through explicit paths\n\nExample: IMS (Information Management System) by IBM\nLimitations:\n  • Difficulty handling many-to-many relationships\n  • Data redundancy when representing complex relationships\n  • Limited query flexibility\n  • Reorganization is difficult\n\n2. NETWORK MODEL (1970s):\n\n![Network Data Model](/images/unit1/network%20data%20model.png)\n\nStructure: Graph-based with records (nodes) and sets (edges)\nCharacteristics:\n  • Many-to-many (M:N) relationships supported\n  • Records connected through pointers\n  • Set construct for 1:N relationships\n  • CODASYL (Conference on Data Systems Languages) standard\n\nExample: IDMS (Integrated Database Management System)\nAdvantages over Hierarchical:\n  • More flexible relationship representation\n  • Better for complex queries\nLimitations:\n  • Complex navigation logic\n  • Difficult to modify structure\n  • Application programs tightly coupled with database structure\n\n3. RELATIONAL MODEL (1970-present):\n\n![Relational Data Model](/images/unit1/relational%20data%20model.png)\n\nFoundation: E.F. Codd's relational model (1970)\nStructure: Data organized in tables (relations)\n\nCore Concepts:\n  • Relation: Table with rows (tuples) and columns (attributes)\n  • Domain: Set of allowed values for an attribute\n  • Primary Key: Unique identifier for tuples\n  • Foreign Key: Reference to primary key in another relation\n\nCharacteristics:\n  • Simple tabular structure\n  • Data independence (logical and physical)\n  • Declarative query language (SQL)\n  • Mathematical foundation (relational algebra/calculus)\n  • ACID properties (Atomicity, Consistency, Isolation, Durability)\n\nAdvantages:\n  • Structural independence: Changes in structure don't affect applications\n  • Ad hoc query capability: Flexible data retrieval\n  • Data integrity through constraints\n  • Normalization to reduce redundancy\n\nExamples: MySQL, PostgreSQL, Oracle, SQL Server, DB2\n\nCOMPARATIVE ANALYSIS:\n\nFeature          | Hierarchical | Network  | Relational\n-----------------|--------------|----------|-------------\nStructure        | Tree         | Graph    | Tables\nRelationships    | 1:N          | M:N      | Any\nNavigation       | Procedural   | Procedural| Declarative\nFlexibility      | Low          | Medium   | High\nQuery Language   | Proprietary  | Proprietary| SQL (Standard)\nData Independence| Low          | Low      | High\nPerformance      | Fast         | Fast     | Moderate\n\nWhy Relational Model Dominates:\n1. Simplicity: Easy to understand and use\n2. Flexibility: Ad hoc queries without predefined access paths\n3. Data Independence: Separation of logical and physical views\n4. Standard Language: SQL is universally adopted\n5. Mathematical Foundation: Formal query optimization\n6. ACID Guarantees: Reliable transaction processing",
          examples: "EXAMPLE 1: HIERARCHICAL DATABASE\n\nUniversity Structure (Tree):\n                University\n                    |\n        -------------------------\n        |                       |\n    Department              Department\n   (Computer Science)        (Mathematics)\n        |                       |\n    ----------              ----------\n    |        |              |        |\n  Faculty  Students      Faculty  Students\n\nLimitation: A student double-majoring requires duplication or complex workarounds.\n\nEXAMPLE 2: NETWORK DATABASE\n\nStudent-Course Registration (Graph):\nStudents <-------- Enrollment --------> Courses\n   |                    |                   |\n   |              (Properties: Grade,       |\n   |               Semester)                |\n   |                                        |\nDepartments                          Instructors\n\nNetwork allows M:N relationships directly but requires complex pointer navigation.\n\nEXAMPLE 3: RELATIONAL DATABASE\n\nUniversity Schema (Tables):\n\nSTUDENTS Table:\nStudentID | Name        | Major    | Email\n----------|-------------|----------|------------------\n101       | John Doe    | CS       | john@uni.edu\n102       | Jane Smith  | Math     | jane@uni.edu\n103       | Bob Johnson | CS       | bob@uni.edu\n\nCOURSES Table:\nCourseID | CourseName           | Credits | Department\n---------|---------------------|---------|------------\nCS101    | Intro to Programming | 3       | CS\nCS202    | Database Systems     | 4       | CS\nMATH101  | Calculus I          | 4       | Math\n\nENROLLMENT Table (Junction):\nStudentID | CourseID | Semester | Grade\n----------|----------|----------|------\n101       | CS101    | Fall2024 | A\n101       | CS202    | Fall2024 | B+\n102       | MATH101  | Fall2024 | A\n103       | CS101    | Fall2024 | B\n\nSQL Query Examples:\n\n-- Find all students enrolled in Database Systems\nSELECT S.Name, S.Email\nFROM STUDENTS S\nJOIN ENROLLMENT E ON S.StudentID = E.StudentID\nJOIN COURSES C ON E.CourseID = C.CourseID\nWHERE C.CourseName = 'Database Systems';\n\n-- Count enrollments per course\nSELECT C.CourseName, COUNT(E.StudentID) as Enrollment_Count\nFROM COURSES C\nLEFT JOIN ENROLLMENT E ON C.CourseID = E.CourseID\nGROUP BY C.CourseName;\n\nREAL-WORLD APPLICATIONS:\n\n1. Banking Systems: Account management, transactions, customer data\n   • Relational model ensures ACID properties for financial transactions\n   • Complex queries for fraud detection and reporting\n\n2. Airline Reservation Systems: Flight schedules, bookings, passenger info\n   • High concurrency handling thousands of simultaneous bookings\n   • Complex relationships between flights, passengers, seats\n\n3. E-Commerce Platforms: Products, orders, customers, inventory\n   • Flexible schema for diverse product catalogs\n   • Transaction support for payment processing\n\n4. Healthcare Systems: Patient records, appointments, prescriptions\n   • Data integrity critical for patient safety\n   • HIPAA compliance through access control",
          practical: "PRACTICAL USAGE AND IMPLEMENTATION:\n\n1. Choosing the Right Model:\n   • Use Relational for: General-purpose applications, ACID requirements, complex queries\n   • Consider NoSQL (modern evolution) for: Massive scale, flexible schema, specific access patterns\n\n2. Database Design Process:\n   Step 1: Requirements Analysis - Understand what data to store and how it will be used\n   Step 2: Conceptual Design - Create ER diagram showing entities and relationships\n   Step 3: Logical Design - Convert ER to relational schema with normalization\n   Step 4: Physical Design - Optimize with indexes, partitioning, clustering\n\n3. Real Implementation Example (MySQL):\n\n-- Create database\nCREATE DATABASE UniversityDB;\nUSE UniversityDB;\n\n-- Create tables with constraints\nCREATE TABLE Students (\n    StudentID INT PRIMARY KEY AUTO_INCREMENT,\n    Name VARCHAR(100) NOT NULL,\n    Email VARCHAR(100) UNIQUE,\n    Major VARCHAR(50),\n    EnrollmentDate DATE DEFAULT CURRENT_DATE\n);\n\nCREATE TABLE Courses (\n    CourseID VARCHAR(10) PRIMARY KEY,\n    CourseName VARCHAR(100) NOT NULL,\n    Credits INT CHECK (Credits > 0),\n    Department VARCHAR(50)\n);\n\nCREATE TABLE Enrollment (\n    StudentID INT,\n    CourseID VARCHAR(10),\n    Semester VARCHAR(20),\n    Grade CHAR(2),\n    PRIMARY KEY (StudentID, CourseID, Semester),\n    FOREIGN KEY (StudentID) REFERENCES Students(StudentID)\n        ON DELETE CASCADE,\n    FOREIGN KEY (CourseID) REFERENCES Courses(CourseID)\n        ON UPDATE CASCADE\n);\n\n4. Industry Best Practices:\n   • Always use foreign keys to maintain referential integrity\n   • Create indexes on frequently queried columns\n   • Regular backups and disaster recovery plans\n   • Monitor performance and optimize queries\n   • Implement proper access control and authentication\n\n5. Career Relevance:\n   • Database Administrator (DBA): Manages database systems, ensures performance and security\n   • Data Engineer: Designs and builds data pipelines\n   • Backend Developer: Implements database interactions in applications\n   • Data Analyst: Writes complex SQL queries for business insights",
          exam: "IMPORTANT EXAM QUESTIONS:\n\n1. Define Database and DBMS. What are the main characteristics of the database approach?\n   Answer: Cover self-describing nature, program-data independence, multiple views, sharing\n\n2. List and explain the advantages of using a DBMS over file systems.\n   Key points: Redundancy control, restricted access, persistent storage, backup/recovery, integrity constraints\n\n3. Compare and contrast Hierarchical, Network, and Relational database models.\n   Create table showing: Structure, relationships supported, advantages, disadvantages, examples\n\n4. Why did the Relational Model become dominant over Hierarchical and Network models?\n   Focus on: Simplicity, data independence, SQL standardization, flexibility\n\n5. Explain the concept of data independence with examples.\n   Distinguish between logical and physical data independence\n\n6. What are the disadvantages of file-based systems that led to database systems?\n   Points: Data redundancy, inconsistency, difficulty in access, integrity problems, atomicity issues, security problems\n\n7. Describe the structure and limitations of the Hierarchical database model.\n   Include: Tree structure, 1:N relationships, navigation, IMS example\n\n8. Draw a simple example showing how M:N relationships are handled differently in Hierarchical, Network, and Relational models.\n\n9. What is a view in a database? Why are multiple views important?\n   Explain: Security, customization, simplicity for different user groups\n\n10. Explain the term 'self-describing' nature of a database system.\n    Focus on: System catalog, metadata, data dictionary\n\nQUICK REVISION POINTS:\n• Database = Collection of related data\n• DBMS = Software to manage database\n• Hierarchical = Tree (1:N only)\n• Network = Graph (M:N supported)\n• Relational = Tables (Most flexible)\n• SQL = Standard query language for relational databases\n• Data Independence = Change structure without affecting programs",
          takeaways: "KEY TAKEAWAYS:\n\n✓ A database is a collection of related data; DBMS is the software that manages it\n\n✓ Database approach provides: data independence, controlled redundancy, concurrent access, backup/recovery, security\n\n✓ Three major historical models: Hierarchical (tree), Network (graph), Relational (tables)\n\n✓ Hierarchical model: Simple but limited to 1:N relationships, used in IBM's IMS\n\n✓ Network model: More flexible with M:N relationships but complex navigation\n\n✓ Relational model: Dominant today due to simplicity, SQL standardization, and data independence\n\n✓ Relational databases organize data in tables with rows (tuples) and columns (attributes)\n\n✓ SQL provides declarative querying - specify WHAT you want, not HOW to get it\n\n✓ Modern applications almost exclusively use relational or NoSQL (evolution of database concepts)\n\n✓ Understanding database evolution helps appreciate why certain design decisions were made\n\n✓ Database systems are critical infrastructure for: banking, healthcare, e-commerce, government, education\n\n✓ Career opportunities: DBA, Data Engineer, Backend Developer, Data Analyst all require database knowledge\n\nREMEMBER: The shift from Hierarchical → Network → Relational represents increasing flexibility and decreasing complexity for users, while the system handles complexity internally.\n\n![ER Model Overview](/images/unit1/er%20model.png)\n\n![Object-Oriented Data Model](/images/unit1/object%20oriented%20data%20model.png)"
        }
      },
      {
        id: "db-architecture",
        title: "Database System Architecture",
        subtopics: ["Data abstraction", "Data independence", "DDL", "DML", "DCL"],
        clos: ["CLO02"],
        cos: ["CO01"],
        content: {
          introduction: "Database architecture defines how a database is structured, stored, managed, and accessed. The architecture provides different views to different users and handles the complexity of physical storage while presenting a simple logical view. Understanding this layered architecture is fundamental to designing scalable, maintainable database systems that can evolve with changing requirements without disrupting applications.",
          concept: "A database system is divided into modules that deal with different responsibilities. The overall system structure consists of various components that interact to provide the database services.\n\n### Three-Schema Architecture (ANSI-SPARC)\n\n![Three Level Schema Architecture](/images/unit1/Three%20level%20schema%20architecture%20of%20database.png)\n\n![Levels of Abstraction](/images/unit1/levels%20of%20abstraction.png)\n\nThe three-schema architecture separates user applications from the physical database through three levels of abstraction:\n\n1. EXTERNAL LEVEL (View Level):\n   • Highest level of abstraction\n   • Describes WHAT data is seen by individual users\n   • Multiple external schemas (views) exist\n   • Each view describes the portion of database relevant to a particular user group\n   • Hides irrelevant details and restricts access\n   • Different users can have completely different views\n\nExample: HR Manager sees employee salaries, regular employee doesn't.\n\n2. CONCEPTUAL LEVEL (Logical Level):\n   • Community view of the database\n   • Describes WHAT data is stored and relationships among data\n   • Single conceptual schema for entire database\n   • Hides details of physical storage\n   • Focus on entities, data types, relationships, constraints\n   • Used by Database Administrators (DBAs)\n\nExample: Complete schema with all tables, relationships, constraints\n\n3. INTERNAL LEVEL (Physical Level):\n   • Lowest level of abstraction\n   • Describes HOW data is physically stored\n   • Deals with: Storage structures, indexes, access paths, file organization\n   • Concerned with: Storage space allocation, data compression, encryption\n   • Used by system programmers and DBAs\n\nExample: B-tree indexes, heap files, hash buckets, block sizes\n\nMAPPINGS BETWEEN LEVELS:\n• External/Conceptual Mapping: Transforms external view requests to conceptual schema\n• Conceptual/Internal Mapping: Maps conceptual schema to physical storage\n\n### Data Independence\n\nThe capacity to change schema at one level without affecting schema at the next higher level.\n\n1. LOGICAL DATA INDEPENDENCE:\n   • Ability to change conceptual schema without changing external schemas or application programs\n   • Changes might include: Adding/removing attributes, creating/removing tables, changing constraints\n   • External/Conceptual mapping handles the conversion\n   • More difficult to achieve than physical independence\n\nExample: Adding a new table or column doesn't affect existing views.\n\n2. PHYSICAL DATA INDEPENDENCE:\n   • Ability to change internal schema without changing conceptual schema\n   • Changes might include: Using different file organization, creating/dropping indexes, changing storage devices, modifying data compression\n   • Conceptual/Internal mapping handles the conversion\n   • Easier to achieve than logical independence\n\nExample: Creating an index on a column doesn't affect queries.\n\nBENEFITS OF DATA INDEPENDENCE:\n• Application programs don't need to be rewritten when database structure changes\n• Physical storage can be optimized without affecting applications\n• Different users can view data differently\n• Security: Users only see authorized data\n• Maintenance and evolution of database is easier",
          technicalDepth: "DATABASE LANGUAGES:\n\nDatabase systems provide specialized languages for different operations:\n\n1. DATA DEFINITION LANGUAGE (DDL):\n\nUsed to define and modify database structure (schema).\n\nKey DDL Commands:\n• CREATE: Define new database objects (tables, indexes, views)\n• ALTER: Modify existing database objects\n• DROP: Delete database objects\n• TRUNCATE: Remove all records from a table\n• RENAME: Rename database objects\n\nDDL Compiler:\n• Processes DDL statements\n• Generates metadata stored in data dictionary (system catalog)\n• Data dictionary contains: Table definitions, attribute definitions, constraints, indexes, storage information\n\nDDL Features:\n• Specify storage structure and access methods\n• Define integrity constraints (primary keys, foreign keys, check constraints)\n• Define security and authorization information\n• Specify physical storage parameters\n\n2. DATA MANIPULATION LANGUAGE (DML):\n\nUsed to access and manipulate data in the database.\n\nTypes of DML:\n\nA. PROCEDURAL DML:\n   • User specifies WHAT data is needed AND HOW to get it\n   • Requires step-by-step navigation\n   • Examples: Hierarchical and Network model languages\n\nB. NON-PROCEDURAL (DECLARATIVE) DML:\n   • User specifies WHAT data is needed without specifying HOW\n   • System determines efficient way to access data\n   • Easier to learn and use\n   • Example: SQL (Structured Query Language)\n\nKey DML Commands (SQL):\n• SELECT: Retrieve data from database\n• INSERT: Add new records\n• UPDATE: Modify existing records\n• DELETE: Remove records\n\nDML Processing:\n• DML Compiler: Translates DML statements\n• Query Optimizer: Determines efficient execution strategy\n• Query Evaluation Engine: Executes the optimized query plan\n\n3. DATA CONTROL LANGUAGE (DCL):\n\nUsed to control access to data in the database.\n\nKey DCL Commands:\n• GRANT: Give user access privileges\n• REVOKE: Remove user access privileges\n\nDCL Controls:\n• User authentication\n• Authorization levels\n• Role-based access control\n• Object-level permissions\n\n4. TRANSACTION CONTROL LANGUAGE (TCL):\n\nManages transactions in the database.\n\nKey TCL Commands:\n• COMMIT: Save transaction changes permanently\n• ROLLBACK: Undo transaction changes\n• SAVEPOINT: Create points within transaction to rollback to\n\n### Database System Components\n\n![Architecture of DBMS](/images/unit1/Architecture%20of%20%20Database%20Management%20System.jpg)\n\n![DBMS Architecture](/images/unit1/DBMS%20Architecture%20%28Centralized%3AShared-Memory%29.svg)\n\n1. STORAGE MANAGER:\n   • Interface between low-level data and application programs\n   • Translates DML statements to low-level file-system commands\n   • Responsible for: Storing, retrieving, updating data\n   \n   Components:\n   • Authorization and Integrity Manager\n   • Transaction Manager\n   • File Manager\n   • Buffer Manager\n\n2. QUERY PROCESSOR:\n   • Simplifies and facilitates data access\n   \n   Components:\n   • DDL Interpreter: Interprets DDL statements\n   • DML Compiler: Translates DML to query evaluation plan\n   • Query Optimizer: Chooses lowest-cost evaluation plan\n   • Query Evaluation Engine: Executes query plan\n\n3. DATABASE USERS:\n\n![Database Users](/images/unit1/Database%20users.svg)\n\n![Query Processing](/images/unit1/query%20processing.png)\n\n   A. Naive Users: Invoke application programs (ATM users, airline booking)\n   B. Application Programmers: Write application programs using DML\n   C. Sophisticated Users: Use query tools, don't write programs (analysts)\n   D. Database Administrator (DBA): Central control over system\n\n4. DBA RESPONSIBILITIES:\n   • Schema definition\n   • Storage structure and access method definition\n   • Schema and physical organization modification\n   • Granting authorization for data access\n   • Routine maintenance: Backups, performance monitoring, security\n\nCLIENT-SERVER ARCHITECTURE:\n\nTwo-Tier Architecture:\n• Client: Application program, user interface\n• Server: Database system (query processing, transaction management)\n• Communication via database protocols (ODBC, JDBC)\n\nThree-Tier Architecture:\n• Client: Presentation layer (browser, mobile app)\n• Application Server: Business logic layer (web server, API)\n• Database Server: Data layer\n• Benefits: Scalability, maintainability, security\n\nModern web applications typically use three-tier architecture.",
          examples: "EXAMPLE 1: DDL STATEMENTS (Creating Database Schema)\n\n-- Create a database\nCREATE DATABASE UniversityDB;\nUSE UniversityDB;\n\n-- Create Department table\nCREATE TABLE Department (\n    DeptID INT PRIMARY KEY AUTO_INCREMENT,\n    DeptName VARCHAR(100) NOT NULL UNIQUE,\n    Building VARCHAR(50),\n    Budget DECIMAL(12,2) CHECK (Budget > 0)\n);\n\n-- Create Student table with constraints\nCREATE TABLE Student (\n    StudentID INT PRIMARY KEY,\n    FirstName VARCHAR(50) NOT NULL,\n    LastName VARCHAR(50) NOT NULL,\n    Email VARCHAR(100) UNIQUE,\n    DeptID INT,\n    EnrollmentDate DATE DEFAULT CURRENT_DATE,\n    GPA DECIMAL(3,2) CHECK (GPA >= 0.0 AND GPA <= 4.0),\n    FOREIGN KEY (DeptID) REFERENCES Department(DeptID)\n        ON DELETE SET NULL\n        ON UPDATE CASCADE\n);\n\n-- Create index for faster queries\nCREATE INDEX idx_student_lastname ON Student(LastName);\n\n-- Alter table to add new column\nALTER TABLE Student\nADD COLUMN PhoneNumber VARCHAR(15);\n\n-- Modify column definition\nALTER TABLE Student\nMODIFY COLUMN Email VARCHAR(150);\n\n-- Drop index\nDROP INDEX idx_student_lastname ON Student;\n\n-- Rename table\nRENAME TABLE Student TO Students;\n\nEXAMPLE 2: DML STATEMENTS (Data Manipulation)\n\n-- INSERT: Adding new records\nINSERT INTO Department (DeptName, Building, Budget)\nVALUES ('Computer Science', 'Tech Building', 500000.00);\n\nINSERT INTO Department VALUES \n    (NULL, 'Mathematics', 'Science Hall', 300000.00),\n    (NULL, 'Physics', 'Science Hall', 450000.00);\n\nINSERT INTO Students (StudentID, FirstName, LastName, Email, DeptID, GPA)\nVALUES (101, 'John', 'Doe', 'john.doe@uni.edu', 1, 3.75);\n\n-- SELECT: Retrieving data\n-- Simple query\nSELECT * FROM Students;\n\n-- With WHERE clause\nSELECT FirstName, LastName, GPA\nFROM Students\nWHERE GPA > 3.5;\n\n-- With JOIN\nSELECT S.FirstName, S.LastName, D.DeptName\nFROM Students S\nJOIN Department D ON S.DeptID = D.DeptID;\n\n-- With aggregation\nSELECT D.DeptName, COUNT(S.StudentID) as StudentCount, AVG(S.GPA) as AvgGPA\nFROM Department D\nLEFT JOIN Students S ON D.DeptID = S.DeptID\nGROUP BY D.DeptName\nHAVING COUNT(S.StudentID) > 0;\n\n-- UPDATE: Modifying existing records\nUPDATE Students\nSET GPA = 3.85\nWHERE StudentID = 101;\n\n-- Update with calculation\nUPDATE Department\nSET Budget = Budget * 1.10\nWHERE DeptName = 'Computer Science';\n\n-- DELETE: Removing records\nDELETE FROM Students\nWHERE EnrollmentDate < '2020-01-01';\n\n-- Delete all records (but keep table structure)\nDELETE FROM Students;\n-- Or use TRUNCATE (faster)\nTRUNCATE TABLE Students;\n\nEXAMPLE 3: DCL STATEMENTS (Access Control)\n\n-- Create users\nCREATE USER 'student_user'@'localhost' IDENTIFIED BY 'password123';\nCREATE USER 'faculty_user'@'localhost' IDENTIFIED BY 'securepass456';\n\n-- GRANT privileges\n-- Give SELECT access to student\nGRANT SELECT ON UniversityDB.Students TO 'student_user'@'localhost';\n\n-- Give multiple privileges to faculty\nGRANT SELECT, INSERT, UPDATE ON UniversityDB.* TO 'faculty_user'@'localhost';\n\n-- Grant with ability to grant to others\nGRANT ALL PRIVILEGES ON UniversityDB.* TO 'admin_user'@'localhost' \nWITH GRANT OPTION;\n\n-- Create role and assign privileges\nCREATE ROLE 'data_analyst';\nGRANT SELECT ON UniversityDB.* TO 'data_analyst';\nGRANT 'data_analyst' TO 'analyst_user'@'localhost';\n\n-- REVOKE privileges\nREVOKE INSERT, UPDATE ON UniversityDB.Students FROM 'faculty_user'@'localhost';\n\n-- Revoke all privileges\nREVOKE ALL PRIVILEGES ON UniversityDB.* FROM 'student_user'@'localhost';\n\nEXAMPLE 4: TCL STATEMENTS (Transaction Control)\n\n-- Bank transfer example demonstrating transaction\nSTART TRANSACTION;\n\n-- Deduct from account A\nUPDATE Accounts\nSET Balance = Balance - 500\nWHERE AccountID = 'A123';\n\n-- Add to account B\nUPDATE Accounts\nSET Balance = Balance + 500\nWHERE AccountID = 'B456';\n\n-- Check if any account went negative\nIF (SELECT MIN(Balance) FROM Accounts WHERE AccountID IN ('A123', 'B456')) < 0 THEN\n    ROLLBACK;  -- Undo both updates\n    SELECT 'Transaction failed: Insufficient funds' AS Message;\nELSE\n    COMMIT;    -- Make changes permanent\n    SELECT 'Transaction successful' AS Message;\nEND IF;\n\n-- Using SAVEPOINT\nSTART TRANSACTION;\n\nINSERT INTO Students VALUES (201, 'Alice', 'Smith', 'alice@uni.edu', 1, 3.5);\nSAVEPOINT sp1;\n\nINSERT INTO Students VALUES (202, 'Bob', 'Johnson', 'bob@uni.edu', 2, 3.2);\nSAVEPOINT sp2;\n\nINSERT INTO Students VALUES (203, 'Charlie', 'Brown', 'charlie@uni.edu', 1, 3.9);\n\n-- Error in last insert, rollback to sp2\nROLLBACK TO SAVEPOINT sp2;\n\n-- Commit the first two inserts\nCOMMIT;\n\nEXAMPLE 5: DATA INDEPENDENCE IN ACTION\n\nSCENARIO: Physical Data Independence\n\n-- Original: No index, slow query\nSELECT * FROM Students WHERE LastName = 'Smith';\n-- Query scans entire table (slow for large tables)\n\n-- DBA creates index (internal level change)\nCREATE INDEX idx_lastname ON Students(LastName);\n\n-- Same query, now fast (uses index)\nSELECT * FROM Students WHERE LastName = 'Smith';\n-- Application code unchanged, but performance improved!\n\nSCENARIO: Logical Data Independence\n\n-- Original view used by application\nCREATE VIEW StudentInfo AS\nSELECT StudentID, FirstName, LastName, Email\nFROM Students;\n\n-- DBA adds new column to base table (conceptual level change)\nALTER TABLE Students ADD COLUMN MiddleName VARCHAR(50);\n\n-- View still works, application unaffected\nSELECT * FROM StudentInfo;\n-- Returns same columns as before",
          practical: "REAL-WORLD APPLICATIONS AND BEST PRACTICES:\n\n1. THREE-TIER WEB APPLICATION EXAMPLE:\n\nPresentation Tier (Frontend):\n  • React/Angular/Vue application\n  • Displays data to user\n  • Sends API requests\n\nApplication Tier (Backend):\n  • Node.js/Python/Java server\n  • Business logic\n  • Validates data\n  • Manages database connections\n  • Uses connection pooling for efficiency\n\nDatabase Tier:\n  • MySQL/PostgreSQL/Oracle\n  • Stores persistent data\n  • Enforces constraints\n  • Handles transactions\n\nExample Code (Node.js + MySQL):\n\nconst mysql = require('mysql2');\n\n// Create connection pool (reuses connections)\nconst pool = mysql.createPool({\n  host: 'localhost',\n  user: 'app_user',\n  password: 'secure_password',\n  database: 'UniversityDB',\n  waitForConnections: true,\n  connectionLimit: 10\n});\n\n// API endpoint using DML\napp.get('/api/students', async (req, res) => {\n  const [rows] = await pool.promise().query(\n    'SELECT StudentID, FirstName, LastName, GPA FROM Students WHERE GPA > ?',\n    [3.0]\n  );\n  res.json(rows);\n});\n\napp.post('/api/students', async (req, res) => {\n  const { firstName, lastName, email, deptId } = req.body;\n  const [result] = await pool.promise().query(\n    'INSERT INTO Students (FirstName, LastName, Email, DeptID) VALUES (?, ?, ?, ?)',\n    [firstName, lastName, email, deptId]\n  );\n  res.json({ studentId: result.insertId, message: 'Student created' });\n});\n\n2. IMPLEMENTING DATA INDEPENDENCE:\n\nPhysical Independence Example:\n-- Application uses view, not table directly\nCREATE VIEW ActiveStudents AS\nSELECT StudentID, FirstName, LastName, Email, GPA\nFROM Students\nWHERE Status = 'Active';\n\n-- DBA can change physical storage without affecting app\nCREATE INDEX idx_status ON Students(Status); -- Add index\nALTER TABLE Students ENGINE=InnoDB; -- Change storage engine\nPARTITION TABLE Students BY RANGE(EnrollmentYear); -- Partition table\n\n-- Application code unchanged:\nSELECT * FROM ActiveStudents; -- Still works!\n\n3. DBA DAILY TASKS:\n\nMorning:\n  • Check database server status\n  • Review overnight backup logs\n  • Monitor disk space usage\n  • Check for slow queries\n\nDuring Day:\n  • Create/modify schemas as requested\n  • Grant/revoke user privileges\n  • Investigate performance issues\n  • Optimize slow queries with indexes\n\nEnd of Day:\n  • Schedule backups\n  • Review security logs\n  • Plan maintenance windows\n\n4. SECURITY BEST PRACTICES:\n\nPrinciple of Least Privilege:\n-- Don't give application full admin access\n-- Create specific user with minimal needed privileges\n\nCREATE USER 'webapp_user'@'%' IDENTIFIED BY 'strong_password';\nGRANT SELECT, INSERT, UPDATE ON UniversityDB.Students TO 'webapp_user'@'%';\nGRANT SELECT ON UniversityDB.Department TO 'webapp_user'@'%';\n-- No DELETE, DROP, or other dangerous operations\n\nUse Views for Security:\n-- Hide sensitive columns\nCREATE VIEW PublicStudentInfo AS\nSELECT StudentID, FirstName, LastName, DeptID\nFROM Students;\n-- SSN, GPA, Address hidden\n\nGRANT SELECT ON UniversityDB.PublicStudentInfo TO 'public_user'@'%';\n\n5. PERFORMANCE OPTIMIZATION:\n\nIndexing Strategy:\n  • Index columns used in WHERE clauses\n  • Index columns used in JOINs\n  • Index columns used in ORDER BY\n  • Don't over-index (slows INSERT/UPDATE)\n\n-- Good index candidates\nCREATE INDEX idx_dept ON Students(DeptID); -- JOIN column\nCREATE INDEX idx_gpa ON Students(GPA);     -- WHERE filter\nCREATE INDEX idx_name ON Students(LastName, FirstName); -- Composite for searches\n\nQuery Optimization:\n  • Use EXPLAIN to analyze query plans\n  • Avoid SELECT *; specify needed columns\n  • Use appropriate JOINs\n  • Consider query caching\n\nEXPLAIN SELECT S.FirstName, S.LastName, D.DeptName\nFROM Students S\nJOIN Department D ON S.DeptID = D.DeptID\nWHERE S.GPA > 3.5;\n\n6. INDUSTRY TOOLS:\n\n  • MySQL Workbench: Visual database design, query tool\n  • pgAdmin: PostgreSQL administration\n  • DBeaver: Universal database tool\n  • DataGrip: JetBrains database IDE\n  • phpMyAdmin: Web-based MySQL administration",
          exam: "IMPORTANT EXAM QUESTIONS:\n\n1. Explain the three-schema architecture of a DBMS with a neat diagram.\n   Answer should include: External, Conceptual, Internal levels with mappings\n\n2. What is data independence? Differentiate between logical and physical data independence with examples.\n   Key points: Definition, two types, examples of each, why important\n\n3. Distinguish between DDL, DML, and DCL with examples of each.\n   Create table showing: Purpose, commands, when used\n\n4. List and explain the components of a database system.\n   Include: Storage Manager, Query Processor, users, DBA\n\n5. What are the responsibilities of a Database Administrator (DBA)?\n   Points: Schema definition, access control, backup/recovery, performance tuning\n\n6. Explain the difference between procedural and non-procedural DML.\n   Examples from different database models\n\n7. Write DDL statements to create a database schema with at least 3 related tables including constraints.\n   Must include: PRIMARY KEY, FOREIGN KEY, CHECK, UNIQUE, NOT NULL\n\n8. Write SQL queries demonstrating all DML operations (SELECT, INSERT, UPDATE, DELETE).\n   Include: Simple and complex queries, joins, aggregation\n\n9. Explain client-server and three-tier database architectures.\n   Draw diagrams, explain components, when to use each\n\n10. How does the concept of data independence help in database maintenance and evolution?\n    Real-world scenarios\n\n11. What is the role of the Data Dictionary (System Catalog)?\n    What information does it store?\n\n12. Write DCL commands to grant and revoke privileges. Explain role-based access control.\n\nQUICK REVISION:\n• External Level = User Views (WHAT users see)\n• Conceptual Level = Logical Schema (WHAT is stored)\n• Internal Level = Physical Storage (HOW stored)\n• DDL = Define structure (CREATE, ALTER, DROP)\n• DML = Manipulate data (SELECT, INSERT, UPDATE, DELETE)\n• DCL = Control access (GRANT, REVOKE)\n• TCL = Manage transactions (COMMIT, ROLLBACK)\n• Physical Independence = Change storage without affecting logic\n• Logical Independence = Change schema without affecting views\n• DBA = Database Administrator (manages everything)",
          takeaways: "KEY TAKEAWAYS:\n\n✓ Three-Schema Architecture separates user views (external), logical structure (conceptual), and physical storage (internal)\n\n✓ Data independence allows changes at one level without affecting other levels - critical for system evolution\n\n✓ Physical data independence: Change storage details (indexes, file organization) without affecting applications\n\n✓ Logical data independence: Change logical schema without affecting user views (harder to achieve)\n\n✓ DDL defines database structure: CREATE, ALTER, DROP, TRUNCATE\n\n✓ DML manipulates data: SELECT (retrieve), INSERT (add), UPDATE (modify), DELETE (remove)\n\n✓ DCL controls access: GRANT (give permissions), REVOKE (remove permissions)\n\n✓ TCL manages transactions: COMMIT (save), ROLLBACK (undo), SAVEPOINT (intermediate points)\n\n✓ Non-procedural (declarative) DML like SQL is easier than procedural - you say WHAT, not HOW\n\n✓ Database system components: Storage Manager, Query Processor, DML Compiler, DDL Compiler\n\n✓ Three-tier architecture (Presentation, Application, Database) is standard for web applications\n\n✓ DBA responsibilities: Schema management, access control, backup/recovery, performance tuning, security\n\n✓ Data dictionary stores metadata: table definitions, constraints, indexes, privileges\n\n✓ Always use parameterized queries to prevent SQL injection attacks\n\n✓ Follow principle of least privilege: grant only necessary permissions\n\nREMEMBER: The layered architecture and data independence are what make modern databases flexible, maintainable, and able to evolve with changing business needs without requiring application rewrites!"
        }
      }
    ]
  },
  {
    id: "unit-2",
    title: "UNIT II: Data Models & Query Languages",
    topics: [
      {
        id: "data-models",
        title: "Data Models",
        subtopics: ["Entity-relationship model", "Network model", "Relational and Object oriented data Models", "Integrity constraints", "Data manipulation operations"],
        clos: ["CLO03"],
        cos: ["CO01"],
        content: {
          introduction: "A data model is a collection of concepts for describing data, relationships between data, and constraints on data. In practice, data models help database designers and users communicate clearly about what must be stored, how it is connected, and what rules must always be true.",
          concept: "In DBMS (as outlined in the master notes), a **data model** has three major parts:\n\n1. **Structure part**: Rules that define how databases can be constructed (tables/records/objects).\n2. **Manipulative part**: Operations allowed on data (queries and updates).\n3. **Integrity rules**: Constraints that keep data accurate and consistent.\n\nData models are commonly grouped as:\n\n- **Object-based / conceptual models**: ER model, Object-oriented model (good for understanding requirements).\n- **Record-based models**: Relational, Network, Hierarchical (used to implement data logically).\n- **Physical data models**: How the system will be implemented on a specific DBMS (tables, keys, indexes, storage details).",
          technicalDepth: "## 1) Entity–Relationship (ER) Model\n\nThe ER model is the most widely used conceptual model for database design because it is easy to understand and maps well to relational tables.\n\n**ER building blocks**\n- **Entity**: real-world object (Student, Course, Account)\n- **Attribute**: property of an entity (StudentID, Name)\n- **Relationship**: association between entities (Student ENROLLS Course)\n\n![ER Symbols (Master Notes)](/images/unit2/ER%20data%20model%20symbol%20table.png)\n\n### Advantages (from master.txt)\n- Simple and easy to build\n- Effective communication tool\n- Easy conversion to relational model\n\n### Disadvantages (from master.txt)\n- No single industry-standard notation\n- Some details can remain hidden because ER is a high-level view\n\n### How to draw ER diagram (master.txt checklist)\n1. Identify entities (rectangles)\n2. Identify relationships (diamonds) and connect entities\n3. Attach attributes to entities\n4. Remove redundant entities/relationships\n5. Finalize constraints clearly (keys, participation, cardinality)\n\n![Cardinality Examples](/images/unit2/er-cardinality-examples.svg)\n\n![Banking ER Example](/images/unit2/E-R%20diagram%20for%20the%20Banking%20Enterprise.png)\n\n## 2) Record-Based Data Models\n\n### Relational model (tables)\n- Data stored as relations (tables); rows = tuples, columns = attributes\n- Relationships maintained using keys (primary/foreign keys)\n\n![Relational Data Model](/images/unit2/relational%20data%20model.png)\n\n### Hierarchical model (tree)\n- Tree structure with single parent for each child\n- Simple and fast traversal, but weak at complex relationships\n\n### Network model (graph)\n- Graph-like structure; a child can have multiple parents\n- Faster access in many cases; more complex to manage\n\n![Network Data Model](/images/unit2/network%20data%20model.png)\n\n## 3) Object-Oriented Data Model\nFrom master.txt: objects combine data + relationships in a single structure; useful for complex data like multimedia.\n\n![Object-Oriented Data Model](/images/unit2/object%20oriented%20data%20model.png)\n\n## 4) Integrity Constraints (must always hold)\n- **Domain constraint**: values must come from an allowed domain (Age between 0–120)\n- **Entity integrity**: primary key cannot be NULL\n- **Referential integrity**: foreign key must match a referenced primary key (or be NULL if allowed)\n\n## 5) Data Manipulation Operations (DML)\n- **SELECT** (read)\n- **INSERT** (add new rows)\n- **UPDATE** (modify rows)\n- **DELETE** (remove rows)",
          examples: "### Example: ER → Relational mapping (student enrollment)\n\n**Entities**\n- Student(StudentID, Name, Email)\n- Course(CourseID, CourseName, Credits)\n\n**Relationship (M:N)**\n- Student ENROLLS Course\n\n**Relational implementation (junction table)**\n\n    CREATE TABLE Student (\n      StudentID INT PRIMARY KEY,\n      Name VARCHAR(100) NOT NULL,\n      Email VARCHAR(150) UNIQUE\n    );\n\n    CREATE TABLE Course (\n      CourseID VARCHAR(10) PRIMARY KEY,\n      CourseName VARCHAR(100) NOT NULL,\n      Credits INT CHECK (Credits > 0)\n    );\n\n    CREATE TABLE Enrollment (\n      StudentID INT,\n      CourseID VARCHAR(10),\n      Semester VARCHAR(20),\n      Grade CHAR(2),\n      PRIMARY KEY (StudentID, CourseID, Semester),\n      FOREIGN KEY (StudentID) REFERENCES Student(StudentID),\n      FOREIGN KEY (CourseID) REFERENCES Course(CourseID)\n    );\n\nThis is the standard way to represent M:N relationships in the relational model.",
          practical: "**Where this is used in real projects**\n\n- ER diagrams are typically created during requirements/design (before coding).\n- Physical data models are then built in tools (e.g., MySQL Workbench) to generate DDL.\n\n**Quick workflow**\n1. Gather requirements → identify entities/relationships\n2. Draw ER → validate with stakeholders\n3. Map ER → relational schema\n4. Add constraints and indexes (physical design)\n5. Implement DDL + DML\n\nTools: MySQL Workbench, draw.io, Lucidchart.",
          exam: "Important questions (Unit II focus)\n\n1. Define data model. Explain structure/manipulative/integrity parts.\n2. Explain ER model with symbols and an example diagram.\n3. Advantages and disadvantages of ER model (from master.txt).\n4. Explain cardinality (1:1, 1:N, M:N) with examples.\n5. Compare Relational vs Hierarchical vs Network models.\n6. Explain integrity constraints: domain, entity integrity, referential integrity.\n7. How is an M:N relationship represented in relational databases?",
          takeaways: "- Data models help represent real-world data clearly and correctly.\n- ER model is the standard conceptual model and converts easily to tables.\n- Integrity constraints protect correctness as data changes.\n- Record-based models define how data is organized logically; physical models focus on DBMS-specific implementation details."
        }
      },
      {
        id: "relational-query-languages",
        title: "Relational Query Languages",
        subtopics: ["Relational algebra", "Tuple and domain relational calculus", "Open source and commercial DBMS"],
        clos: ["CLO04"],
        cos: ["CO02"],
        content: {
          introduction: "Query languages let users request information from a database. In the relational world, the theoretical foundations are **relational algebra (procedural)** and **relational calculus (declarative)**, while SQL is the practical language used in real DBMS products.",
          concept: "From the master notes:\n\n- **Procedural languages**: specify *what* you want and *how* to obtain it (Relational Algebra).\n- **Non-procedural / declarative languages**: specify *what* you want; the system finds *how* (Relational Calculus, SQL style).\n\nRelational algebra is an algebra where **operands are relations** and **operators create new relations** (closure property). This is why it underpins SQL optimization and query execution planning.",
          technicalDepth: "## Relational model (quick base)\n- A relation can be viewed as a set of tuples over domains (D1 × D2 × … × Dn).\n- Attribute values should be atomic (1NF idea); NULL values exist but complicate theory.\n\n## Relational algebra operations (master list)\nBasic operations include: **Select (σ), Project (π), Union (∪), Set Difference (−), Cartesian Product (×)** and **Rename (ρ)**.\n\n![Relational Algebra Operations](/images/unit2/relational-algebra-operations.svg)\n\n### Selection (σ)\nFilters tuples that satisfy a predicate.\nExample idea: σ(branch='Perryridge')(loan)\n\n### Projection (π)\nKeeps only selected attributes and removes duplicates.\nExample idea: π(account_number, balance)(account)\n\n### Composition\nOperations can be combined (e.g., selection after product to form a join-like result).\n\n## Tuple and Domain Relational Calculus\n- **Tuple relational calculus (TRC)**: describes the set of tuples that satisfy a predicate.\n- **Domain relational calculus (DRC)**: uses variables over domains (attribute values).\n\nSQL is closest to tuple relational calculus (declarative), but practical DBMS also perform procedural planning internally.\n\n## Open-source vs commercial DBMS\n- **Open source**: PostgreSQL, MySQL, MariaDB, SQLite\n- **Commercial**: Oracle, Microsoft SQL Server, IBM DB2\n\n(Choice depends on scale, licensing, tooling, and enterprise features.)",
              examples: "### Example 1: Names of students enrolled in DBMS\n\nRelational Algebra:\nπ(Name)(σ(CourseName='DBMS')(Student ⋈ Enrollment ⋈ Course))\n\nSQL (conceptual form):\n\n    SELECT DISTINCT S.Name\n    FROM Student S\n    JOIN Enrollment E ON S.SID = E.SID\n    JOIN Course C ON E.CID = C.CID\n    WHERE C.CourseName = 'DBMS';\n\n### Example 2: Students with no enrollments\n\nRelational Algebra:\nπ(SID)(Student) − π(SID)(Enrollment)\n\n### Example 3: Why union-compatibility matters\nUnion requires same number of attributes and compatible domains.\nExample idea: π(customer_name)(depositor) ∪ π(customer_name)(borrower)",
          practical: "**Practical tips**\n- SQL engines translate queries into internal algebra/plan trees, then optimize join order and access paths.\n- Learning relational algebra makes it easier to understand why indexes and join order affect performance.\n\n**Recommended practice**\n- Write a SQL query → mentally map to σ/π/⋈ operations → simplify by pushing σ early.",
          exam: "Expected exam questions\n\n1. Define relational algebra. List basic operations and their purpose.\n2. Explain selection and projection with examples.\n3. What is union compatibility?\n4. Differentiate relational algebra vs tuple/domain relational calculus.\n5. Convert a SQL query to a relational algebra expression (and vice versa).\n6. Compare open-source and commercial DBMS with examples.",
          takeaways: "- Relational algebra is procedural and forms the basis of query processing/optimization.\n- Selection (σ) reduces rows; projection (π) reduces columns and duplicates.\n- Union/difference require compatible schemas.\n- SQL is practical and largely declarative, but DBMS executes it via optimized internal plans." 
        }
      }
    ]
  },
  {
    id: "unit-3",
    title: "UNIT III: Relational Database Design",
    topics: [
      {
        id: "relational-design",
        title: "Relational Database Design",
        subtopics: ["Domain and data dependency", "Armstrong's axioms", "Functional dependencies", "Normal forms", "Dependency preservation", "Lossless design"],
        clos: ["CLO03"],
        cos: ["CO03"],
        content: {
          introduction: "Database design is the art and science of organizing data to minimize redundancy while ensuring integrity. Poor design leads to data anomalies—inconsistencies that creep in during updates, deletions, or insertions. Normalization is the systematic process that transforms poorly designed schemas into well-structured ones.",
          concept: `Normalization decomposes relations with anomalies into smaller, well-structured relations. The goal is to reduce redundancy and avoid anomalies **without losing information**.

From the master notes, the classic anomalies are:

- **Update anomaly**: the same fact is repeated in multiple rows, so changing it requires multiple updates (and missing one row makes the database inconsistent).
- **Insertion anomaly**: you cannot insert a new fact because another attribute (unrelated to that fact) would be forced to NULL or missing.
- **Deletion anomaly**: deleting one fact unintentionally deletes another important fact.

The engine behind normalization is the **functional dependency (FD)**.

- X → Y means: if two tuples agree on X, they must agree on Y.
- A **candidate key** can be tested using **attribute closure** (the same technique used in the master.txt 2NF/3NF examples).`,
          technicalDepth: `## Functional dependencies and inference

### Armstrong's axioms
- **Reflexivity**: if Y ⊆ X then X → Y
- **Augmentation**: if X → Y then XZ → YZ
- **Transitivity**: if X → Y and Y → Z then X → Z

Common derived rules: **union**, **decomposition**, **pseudotransitivity**.

### Attribute closure (to find keys)
Compute X+ under the FD set. If X+ contains all attributes of R, then X is a superkey; if no proper subset is a superkey, then X is a **candidate key**.

## Normal forms (progressive refinement)

![Normalization Flow](/images/unit3/normalization-flow.svg)

### 1NF
All attribute values are atomic.

### 2NF (master definition)
In 1NF, and **no non-prime attribute** is **partially dependent** on a candidate key (key should not be “broken”).

### 3NF (master definitions)
In 2NF, and **no non-prime attribute** is **transitively dependent** on the key.

Practical FD test used in notes: for every non-trivial X → Y, either:
1) X is a **superkey**, or
2) Y is a **prime attribute**.

### BCNF
For every non-trivial FD X → Y, X must be a superkey.

## Decomposition properties

### Lossless join
Decomposition should reconstruct the original relation without spurious tuples.

### Dependency preservation (master)
In a decomposition R -> R1, R2, ..., every dependency of R must either hold completely inside some decomposed relation, or be derivable from the union of dependencies across the decomposed relations.

Master example: if R(A,B,C,D) has A → BC and we decompose into R1(ABC) and R2(AD), it is dependency-preserving because A → BC is enforced in R1(ABC).`,
          examples: `## Worked example (2NF) — based on master.txt

Given R(A,B,C,D) with FD set { AB → CD, B → C }.

### Step 1: Candidate key (closure method)
- Compute AB+:
  - From AB → CD, we get C and D
  - So AB+ = ABCD (all attributes)
- Therefore **AB is a candidate key**.

### Step 2: Check 2NF
- Prime attributes: A, B
- Non-prime attributes: C, D
- AB → CD is fine (full dependency on the key)
- B → C violates 2NF (a non-prime attribute depends on a proper subset of a composite key)

### Step 3: Convert to 2NF (master decomposition)
- R1(B, C)
- R2(A, B, D)

## Worked example (3NF) — based on master.txt

Given R(X,Y,Z) with FD set { X → Y, Y → Z }.

### Step 1: Candidate key
X+ = XYZ so **X is a candidate key**.

### Step 2: Check 3NF
FD Y → Z violates 3NF because Y is not a superkey and Z is not prime.

### Step 3: Convert to 3NF (master decomposition)
- R1(X, Y)
- R2(Y, Z)`,
          practical: `## Practical design workflow

1) Draft relations from requirements.
2) List likely FDs (business rules, uniqueness constraints).
3) Use **closure** to confirm candidate keys (as in master examples).
4) Normalize to **3NF** for most OLTP designs.
5) Ensure decompositions are **lossless** and preferably **dependency-preserving** (important both in exams and in real enforcement).

### Engineering trade-offs
- Over-normalization increases joins; denormalization may be justified for read-heavy reporting.
- If BCNF breaks dependency preservation and complicates enforcement, 3NF is often the better compromise.
- Enforce FDs using constraints: PK/FK/UNIQUE/CHECK (and sometimes triggers).`,
          exam: `## Exam prompts

1) Find candidate keys using **attribute closure**.
2) Identify whether a relation is in 2NF/3NF/BCNF given FDs.
3) Convert to 2NF/3NF using decomposition (like the master 2NF/3NF worked problems).
4) Define and illustrate update/insertion/deletion anomalies.
5) State Armstrong's axioms and use them to infer an FD.
6) Check decomposition for **lossless join** and **dependency preservation**.
7) Explain the practical difference between 3NF and BCNF.`,
          takeaways: `- Anomalies are the symptom; FDs are the cause.
- Closure is the most reliable tool to reason about keys.
- 2NF removes partial dependency; 3NF removes transitive dependency.
- Good decompositions are **lossless** and ideally **dependency-preserving**.
- 3NF is commonly used in practice; BCNF is stricter but may complicate enforcement.`
        }
      },
      {
        id: "query-processing",
        title: "Query Processing and Optimization",
        subtopics: ["Evaluation of relational algebra expressions", "Query equivalence", "Join strategies", "Query optimization algorithms"],
        clos: ["CLO04"],
        cos: ["CO04"],
        content: {
          introduction: "When you write a SQL query, the database doesn't just blindly execute it. Behind the scenes, the query processor analyzes, transforms, and optimizes your query to find the fastest way to get results. Understanding this process helps you write better queries and diagnose performance issues.",
          concept: `Query processing is the set of activities performed by the DBMS to translate a high-level query (SQL) into an efficient execution plan.

From master.txt, the pipeline is:
1. **Parsing and translation**
2. **Optimization** (the DML compiler chooses the lowest-cost evaluation plan among alternatives)
3. **Evaluation** (execute the chosen plan)

Why it matters: pushing selections/projections early can drastically reduce intermediate results, which often turns “slow” queries into fast ones.`,
          technicalDepth: "Query Optimization Approaches:\n\n1. HEURISTIC OPTIMIZATION (Rule-Based):\n\nRules that generally improve performance:\n\nRule 1: Perform selections early\n  πname(σsalary>50000(Employee ⋈ Department))\n  Better: πname(σsalary>50000(Employee) ⋈ Department)\n  Why? Smaller intermediate result\n\nRule 2: Perform projections early\n  Reduce tuple size, fewer I/Os\n\nRule 3: Combine selections\n  σc1(σc2(R)) = σ(c1 AND c2)(R)\n  Single scan instead of two\n\nRule 4: Combine projections\n  Similar to selections\n\nRule 5: Break complex selections with OR\n  σ(c1 OR c2)(R) can use union of two index scans\n\nRule 6: Evaluate Cartesian products with joins\n  σcondition(R × S) = R ⋈condition S\n  Join algorithms are optimized\n\n2. COST-BASED OPTIMIZATION:\n\nEstimate cost of each plan using:\n\nA. Statistics:\n   - Number of tuples (n)\n   - Tuple size (bytes)\n   - Number of distinct values (V)\n   - Index structure (B-tree depth, hash buckets)\n\nB. Cost Components:\n   - Disk I/O cost (dominant factor)\n   - CPU cost (comparisons, sorting)\n   - Memory usage\n   - Network cost (distributed databases)\n\nC. Selectivity Estimation:\n   Selectivity = fraction of tuples satisfying condition\n\n   For A = value: 1/V(A) (uniform distribution assumption)\n   For A > value: (max - value)/(max - min)\n   For A AND B: s(A) × s(B) (independence assumption)\n   For A OR B: s(A) + s(B) - s(A)×s(B)\n\nJOIN STRATEGIES:\n\n1. Nested Loop Join:\n   For each tuple r in R:\n     For each tuple s in S:\n       If join-condition(r,s): output\n   Cost: n(R) + n(R)×n(S)\n   Best when: One relation is small\n\n2. Block Nested Loop:\n   Process blocks instead of tuples\n   Cost: b(R) + b(R)×b(S)\n   Better I/O performance\n\n3. Index Nested Loop:\n   For each tuple r in R:\n     Use index on S to find matching tuples\n   Cost: n(R) + n(R)×c\n   Where c = cost of index lookup\n   Best when: Index exists on join attribute\n\n4. Sort-Merge Join:\n   Sort both relations on join attribute\n   Merge sorted relations\n   Cost: b(R) + b(S) + sort cost\n   Best for: Large sorted or nearly-sorted relations\n\n5. Hash Join:\n   Build phase: Hash smaller relation\n   Probe phase: Hash larger relation, probe hash table\n   Cost: 3(b(R) + b(S))\n   Best for: Equi-joins on large unsorted relations\n   Most commonly used in practice\n\nQuery Equivalence:\nTwo queries are equivalent if they produce same result for any database state.\n\nEquivalence Rules:\n1. σ(c1 AND c2)(R) = σc1(σc2(R))\n2. σc1(σc2(R)) = σc2(σc1(R))\n3. πL1(πL2(...(πLn(R)))) = πL1(R)\n4. σc(R × S) = R ⋈c S\n5. R ⋈ S = S ⋈ R (commutative)\n6. (R ⋈ S) ⋈ T = R ⋈ (S ⋈ T) (associative)\n\nDynamic Programming for Join Ordering:\nFor n tables, there are (2n-2)!/(n-1)! possible join orders.\nDP finds optimal order in O(n· 2^n) time.",
          examples: "Example 1: Query Transformation\n\nOriginal SQL:\nSELECT name\nFROM Employee, Department\nWHERE Employee.deptId = Department.id\n  AND Department.location = 'NYC'\n  AND Employee.salary > 50000;\n\nNaive Plan:\n1. Employee × Department (huge Cartesian product)\n2. Filter all conditions\n3. Project name\n\nOptimized Plan:\n1. σlocation='NYC'(Department) — filter departments first\n2. σsalary>50000(Employee) — filter employees first\n3. Filtered_Employee ⋈ Filtered_Department — smaller join\n4. πname — project at end\n\nExample 2: Join Strategy Selection\n\nQuery: Employee ⋈deptId Department\n\nScenario A: Employee (1M rows), Department (50 rows)\nBest: Index Nested Loop\n  - Build index on Employee.deptId\n  - For each department, lookup matching employees\n  - Cost: 50 index lookups\n\nScenario B: Employee (1M rows), Department (500K rows)\nBest: Hash Join\n  - Hash smaller relation (Department)\n  - Probe with Employee\n  - Single pass through both relations\n\nExample 3: Cost Estimation\n\nGiven:\n- Employee: 10,000 tuples, 100 tuples/block = 100 blocks\n- Department: 50 tuples, 5 tuples/block = 10 blocks\n- Join: Employee.deptId = Department.id\n\nNested Loop:\n  Cost = 100 + 10,000 × 10 = 100,100 block accesses\n\nBlock Nested Loop:\n  Cost = 100 + 100 × 10 = 1,100 block accesses\n\nHash Join:\n  Cost = 3(100 + 10) = 330 block accesses\n\nWinner: Hash Join\n\nEXPLAIN Command:\n\nEXPLAIN SELECT * FROM Employee WHERE salary > 50000;\n\nOutput shows:\n- Scan type (Sequential, Index, Bitmap)\n- Rows estimate\n- Cost estimate\n- Filter conditions\n- Join method\n\nExample Output:\nSeq Scan on employee (cost=0.00..180.00 rows=1000 width=50)\n  Filter: (salary > 50000)\n\nVs with index:\nIndex Scan using idx_salary (cost=0.00..45.00 rows=1000 width=50)\n  Index Cond: (salary > 50000)",
          practical: "Performance Tuning Tips:\n\n1. Write Efficient Queries:\n   - Use WHERE to filter early\n   - Avoid SELECT *, specify needed columns\n   - Use LIMIT when possible\n   - Avoid functions on indexed columns\n\n2. Indexing Strategy:\n   - Index foreign keys\n   - Index columns in WHERE, JOIN, ORDER BY\n   - Avoid over-indexing (slows INSERT/UPDATE)\n   - Consider composite indexes for multiple columns\n\n3. Understand Query Plans:\n   - Use EXPLAIN to see execution plan\n   - Look for sequential scans on large tables\n   - Check join methods\n   - Monitor rows estimates vs actual\n\n4. Database Configuration:\n   - Buffer pool size (memory for caching)\n   - Work_mem (memory for sorting, hashing)\n   - Statistics collection (ANALYZE command)\n\n5. Query Hints (use sparingly):\n   - Force index usage\n   - Specify join order\n   - Enable/disable certain optimizations\n\nReal-World Scenarios:\n- E-commerce: Optimize product search queries\n- Analytics: Optimize complex aggregation queries\n- Social media: Optimize feed generation queries\n- Financial: Optimize transaction history queries",
          exam: "Expected Questions:\n\n1. Explain phases of query processing with diagram\n2. What is the difference between heuristic and cost-based optimization?\n3. Compare all join strategies: nested loop, block nested loop, index nested loop, sort-merge, hash\n4. Given query and statistics, calculate cost for different join orders\n5. Transform given SQL query to optimized relational algebra\n6. Explain dynamic programming approach for join ordering\n7. What is selectivity? How is it estimated?\n8. Given EXPLAIN output, identify performance bottlenecks\n9. Why is pushing selections down beneficial?\n10. Explain the role of indexes in query optimization",
          takeaways: "Key Concepts:\n\n• Query optimization chooses the fastest execution plan from many alternatives\n• Heuristic rules provide quick optimizations (push selections down)\n• Cost-based optimization uses statistics to estimate plan costs\n• Join strategy selection depends on table sizes and available indexes\n• Hash join is usually best for large equi-joins\n• Index nested loop is best when one table is small\n• Understanding optimization helps write efficient queries\n• EXPLAIN command reveals execution plans for tuning"
        }
      }
    ]
  },
  {
    id: "unit-4",
    title: "UNIT IV: Storage & Transactions",
    topics: [
      {
        id: "storage-strategies",
        title: "Storage Strategies",
        subtopics: ["Indices", "B-trees", "Hashing"],
        clos: ["CLO02"],
        cos: ["CO03"],
        content: {
          introduction: "The gap between memory speed and disk speed is vast—memory access takes nanoseconds, disk access takes milliseconds. That's a million-fold difference. Storage strategies bridge this gap through clever data structures, caching, and organization techniques that minimize disk I/O, the primary bottleneck in database performance.",
          concept: `Storage strategies deal with how data is organized on disk and cached in memory so the DBMS can minimize disk I/O.

From master.txt (disk storage section): large databases rely on disk storage and maintain:
- **Data files** (the database itself)
- **Data dictionary** (metadata / schema)
- **Indices** (fast access paths)

An index provides **pointers** to data items that hold a particular value. Good storage strategies combine buffering + file organization + indexes/hashing to avoid repeated full scans.`,
          technicalDepth: "FILE ORGANIZATION:\n\n1. Heap File (Unordered):\n   Records stored in insertion order\n   Insert: O(1) - append at end\n   Search: O(n) - scan entire file\n   Delete: O(n) - find then mark deleted\n   Best for: Bulk loading, full table scans\n\n2. Sorted File:\n   Records sorted by search key\n   Insert: O(n) - maintain order\n   Search: O(log n) - binary search\n   Range queries: Efficient\n   Best for: Static data with range queries\n\n3. Hash File:\n   Records distributed across buckets using hash function\n   Search: O(1) average - compute hash, go to bucket\n   Range queries: Inefficient (requires full scan)\n   Best for: Exact match lookups\n\nINDEXING STRUCTURES:\n\n1. B-TREE:\n\nProperties:\n- Balanced tree (all leaves at same level)\n- Order d: Each node has [d, 2d] keys (except root)\n- Internal nodes: [d+1, 2d+1] children\n- Keeps data sorted\n\nStructure:\nNode: [P1, K1, P2, K2, ..., Pn, Kn, Pn+1]\n- Ki = keys (sorted)\n- Pi = pointers to children or data\n- All keys in subtree Pi < Ki < all keys in subtree Pi+1\n\nSearch Algorithm:\n1. Start at root\n2. Binary search keys in node\n3. Follow appropriate pointer\n4. Repeat until leaf\n\nInsertion:\n1. Search to find leaf\n2. Insert key in sorted order\n3. If leaf overflows (> 2d keys):\n   - Split into two nodes\n   - Promote middle key to parent\n   - Recursively split parent if needed\n\nDeletion:\n1. Search and remove key\n2. If underflow (< d keys):\n   - Borrow from sibling, OR\n   - Merge with sibling\n   - Recursively fix parent\n\nCost Analysis (n records, order d):\n- Tree height: log_d(n)\n- Search: O(log n) disk I/Os\n- Insert/Delete: O(log n) disk I/Os\n\n2. B+ TREE (Most Common in Databases):\n\nDifferences from B-Tree:\n1. All data in leaf nodes\n2. Internal nodes only store keys + pointers\n3. Leaf nodes linked (sequential access)\n\nAdvantages:\n- Higher fanout (more keys per internal node)\n- Shorter tree (fewer I/Os)\n- Efficient range scans (follow leaf pointers)\n- Sequential access without tree traversal\n\nExample B+ Tree (order d=2):\n                [30]\n               /    \\\n          [10,20]  [40,50]\n         /  |  \\    /  |  \\\nLeaves: [5,7][12,15][25,28][35,38][45,48][55,60]\nLeaves linked: → allows sequential scan\n\n3. HASH INDEX:\n\nStatic Hashing:\nhash(key) % M = bucket number\nIssue: Fixed M, poor with growing data\n\nDynamic Hashing (Extendible Hashing):\n- Bucket directory grows/shrinks\n- Only affected buckets split\n- Efficient for insertions\n\nLinear Hashing:\n- No directory\n- Gradual bucket splitting\n- Handles overflows with chaining\n\nBUFFER MANAGEMENT:\n\nBuffer Pool: Area of memory for caching disk pages\n\nReplacement Policies:\n\n1. LRU (Least Recently Used):\n   Replace page not used for longest time\n   Good general-purpose policy\n\n2. MRU (Most Recently Used):\n   Replace most recently used\n   Good for sequential scans\n\n3. Clock Algorithm:\n   Approximates LRU with less overhead\n   Each page has reference bit\n\n4. LRU-K:\n   Track K most recent accesses\n   Better for database workloads\n\nPin Count: Number of active users of a page\n- Pinned pages cannot be replaced\n- Must unpin after use\n\nDirty Bit: Indicates modified page\n- Must write back before replacement\n- WAL rule: Log before data\n\nCLUSTERING:\n\nStore related records physically close\n\nTypes:\n1. Intra-file clustering: Related records in same file\n2. Inter-file clustering: Records from multiple tables together\n\nClustered Index:\n- Data stored in index order\n- Only one per table\n- Improves range queries\n- Slows insertions (maintain order)\n\nExample: Customer table clustered by CustomerID\n- CustomerID 1-1000 in block 1\n- CustomerID 1001-2000 in block 2\n- Range query efficient",
          examples: "Example 1: B+ Tree Operations\n\nInitial B+ Tree (order 3, max 5 keys per node):\n                     [30]\n                   /      \\\n             [10,20]      [40,50]\n            /   |   \\      /  |  \\\nLeaves: [5,7,9][12,15,18][25,27,29][35,38][45,48][55,60]\n\nInsert 13:\n1. Find correct leaf: [12,15,18]\n2. Insert: [12,13,15,18]\n3. No overflow, done\n\nInsert 14:\n1. Find leaf: [12,13,15,18]\n2. Insert: [12,13,14,15,18]\n3. No overflow, done\n\nInsert 16:\n1. Find leaf: [12,13,14,15,18]\n2. Insert: [12,13,14,15,16,18]\n3. Overflow! Split:\n   Left: [12,13,14]\n   Right: [15,16,18]\n   Promote 15 to parent: [10,15,20]\n\nExample 2: Index Selection\n\nQuery: SELECT * FROM Orders WHERE OrderDate BETWEEN '2024-01-01' AND '2024-01-31';\n\nOption A: No index\n- Full table scan: 1M orders × 100 bytes = 100MB\n- Assume 4KB pages: 25,000 page reads\n\nOption B: B+ Tree index on OrderDate\n- Assume 5% orders in range = 50K orders\n- Tree height: 3 levels\n- Cost: 3 (tree traversal) + 50K/100 (data pages) = 503 page reads\n- 50x faster!\n\nExample 3: Hash vs B+ Tree\n\nExact Match Query:\nSELECT * FROM Employee WHERE EmployeeID = 12345;\n\nHash Index:\n- hash(12345) % 100 = 45\n- Go directly to bucket 45\n- Cost: 1-2 I/Os (bucket + maybe overflow)\n\nB+ Tree Index:\n- Traverse tree: log_d(n) = 3 levels\n- Cost: 3 I/Os\n\nWinner: Hash (for exact match)\n\nRange Query:\nSELECT * FROM Employee WHERE EmployeeID BETWEEN 10000 AND 20000;\n\nHash Index:\n- No ordering information\n- Must scan all buckets\n- Cost: All buckets\n\nB+ Tree Index:\n- Find 10000, then sequential scan\n- Cost: log_d(n) + result size\n\nWinner: B+ Tree (for range)\n\nSQL Index Creation:\n\nCREATE INDEX idx_employee_dept ON Employee(DepartmentID);\nCREATE UNIQUE INDEX idx_employee_email ON Employee(Email);\nCREATE INDEX idx_order_date ON Orders(OrderDate);\n\n-- Composite index\nCREATE INDEX idx_employee_name ON Employee(LastName, FirstName);\n\n-- Covering index (includes non-key columns)\nCREATE INDEX idx_employee_salary ON Employee(DepartmentID) INCLUDE (Salary);\n\n-- Partial index (PostgreSQL)\nCREATE INDEX idx_active_orders ON Orders(OrderDate) WHERE Status='Active';",
          practical: "Best Practices:\n\n1. Index Strategy:\n   - Index foreign keys (join performance)\n   - Index WHERE clause columns\n   - Index ORDER BY columns\n   - Don't over-index (slows writes)\n   - Monitor index usage (drop unused)\n\n2. When to Use Each Index Type:\n   - B+ Tree: Default choice, handles all queries\n   - Hash: Exact match on high-cardinality columns\n   - Bitmap: Low-cardinality columns (gender, status)\n   - Full-text: Text search\n   - Spatial: Geographic data\n\n3. Clustering Decisions:\n   - Cluster on most frequent range query column\n   - Consider write vs read trade-off\n   - Only one clustered index per table\n\n4. Buffer Pool Tuning:\n   - PostgreSQL: shared_buffers (25% of RAM)\n   - MySQL: innodb_buffer_pool_size (70-80% of RAM)\n   - Monitor cache hit ratio (aim for >99%)\n\n5. Storage Hardware Considerations:\n   - SSD: Lower latency, random I/O less painful\n   - HDD: Optimize for sequential access\n   - Consider NVMe for extremely high throughput\n\nReal-World Scenarios:\n- E-commerce: Index product searches, cluster orders by date\n- Social media: Index user queries, hash indexes for user IDs\n- Analytics: Columnar storage, heavy compression\n- Banking: Cluster transactions by account, index timestamps",
          exam: "Important Questions:\n\n1. Compare heap, sorted, and hash file organizations. When to use each?\n2. Explain B+ Tree structure. Why preferred over B-Tree?\n3. Show step-by-step insertion/deletion in B+ Tree\n4. Calculate cost of query with and without index\n5. Explain buffer replacement policies: LRU, MRU, Clock\n6. What is a clustered index? Pros and cons?\n7. Compare hash index vs B+ Tree index\n8. Explain extendible hashing. How does it handle growth?\n9. What factors determine index selectivity?\n10. Design indexing strategy for given schema and query workload",
          takeaways: "Key Principles:\n\n• Disk I/O is the primary bottleneck; minimize it with indexes and caching\n• B+ Tree is the default index: handles all query types, keeps data sorted\n• Hash indexes excel at exact matches but can't do ranges\n• Clustered indexes physically order data, improving range queries\n• Buffer pool caches hot pages, dramatically improving performance\n• Over-indexing hurts write performance; balance reads vs writes\n• Modern SSDs change trade-offs but don't eliminate I/O concerns"
        }
      },
      {
        id: "transaction-processing",
        title: "Transaction Processing",
        subtopics: ["Concurrency control", "ACID property", "Serializability", "Schedules", "Locking and timestamp based schedules", "Multi-version and optimistic concurrency control schemes", "Database recovery"],
        clos: ["CLO05"],
        cos: ["CO05"],
        content: {
          introduction: "Imagine transferring $500 from your savings to checking account. The bank's system must: (1) deduct $500 from savings, (2) add $500 to checking. What if the system crashes between steps? You'd lose $500! Transactions ensure that either both steps happen, or neither does—no in-between states.",
          concept: `From master.txt:

A **transaction** is a single logical unit of work that accesses and possibly modifies the contents of a database using read and write operations.

DBMS enforces **ACID**:
- **Atomicity**: all-or-nothing; if a transaction **aborts**, its changes are not visible; if it **commits**, its changes become visible.
- **Consistency**: integrity constraints remain satisfied before and after the transaction.
- **Isolation**: changes in one transaction are not visible to others until commit.
- **Durability**: once committed, updates persist on disk even after failures.

This is why a money transfer must either complete both updates or roll back completely if the system fails mid-way.`,
          technicalDepth: `## Transaction states (master)

- **Active**: instructions are executing
- **Partially committed**: all operations done, changes in buffer/main memory
- **Committed**: changes made permanent in the database
- **Failed**: an instruction fails
- **Aborted**: rollback to undo partial effects
- **Terminated**: system is consistent and ready for new transactions

## Schedules (master)

### Serial schedule
No transaction starts until the running transaction ends.

### Concurrent (non-serial) schedule
Operations of multiple transactions are interleaved. This improves CPU utilization but can lead to inconsistency if not controlled.

The notes classify concurrent schedules into **serializable** and **non-serializable**.

## Serializability (master)

### Conflict serializability
Conflicting operations satisfy all:
1) belong to different transactions
2) operate on the same data item
3) at least one is a write

To test conflict serializability using a **precedence graph**:
1) create a node for each transaction
2) add an edge Ti -> Tj when an operation of Ti must occur before a conflicting operation of Tj
3) schedule is conflict-serializable iff the graph has **no cycle**

### View serializability
Two schedules are view equivalent if they preserve:
1) initial reads
2) final writes
3) updated reads

## Concurrency-control techniques (master)

### Lock-based protocols

Lock modes:
- **Shared (S)**: read
- **Exclusive (X)**: read + write

**Two-phase locking (2PL)** ensures conflict-serializable schedules:

![Two-Phase Locking (2PL) Phases](/images/unit4/2pl-phases.svg)

- Phase 1 (growing): acquire locks only
- Phase 2 (shrinking): release locks only

Notes highlight common drawbacks: deadlocks and cascading rollback.

Strict / rigorous 2PL (master):
- **Strict 2PL**: hold exclusive locks till commit/abort
- **Rigorous 2PL**: hold all locks till commit/abort (serial order matches commit order)

### Timestamp-based protocols
Each transaction gets a timestamp; the timestamp order defines the serial order.

Maintain per item Q:
- WTS(Q): largest timestamp of any successful write(Q)
- RTS(Q): largest timestamp of any successful read(Q)

Rules from the notes:
- read(Q): reject and rollback if TS(Ti) <= WTS(Q); else allow and set RTS(Q) = max(RTS(Q), TS(Ti))
- write(Q): reject and rollback if TS(Ti) < RTS(Q) or TS(Ti) < WTS(Q); else allow and set WTS(Q) = TS(Ti)

### Validation-based (optimistic) protocol
Three phases:
1) read/execute to local variables
2) validation
3) write phase if validated; otherwise rollback

## Recoverability and recovery (master)

### Recoverable vs irrecoverable schedules
- **Recoverable**: if a transaction performs a dirty read, its commit is delayed until the writer commits or aborts.
- **Irrecoverable**: a transaction commits after reading an uncommitted value that is later rolled back.

Kinds mentioned: **cascading**, **cascadeless**, and **strict** schedules.

### Failures
- transaction failure (logical/system)
- system crash
- disk failure / physical problems

### Logs, undo, redo
System log stores transaction boundaries and update records.

- Undo of <Ti, X, V1, V2> restores old value V1.
- Redo restores new value V2.

Recovery techniques in the notes:
- **Deferred update**: no-undo/redo (redo at commit)
- **Immediate update**: undo/redo (log is written before applying updates)`,
          examples: "Example 1: Transaction in SQL\n\n-- Bank Transfer: Savings to Checking\nBEGIN TRANSACTION;\n\n-- Step 1: Deduct from savings\nUPDATE Account\nSET Balance = Balance - 500\nWHERE AccountID = 101 AND AccountType = 'Savings';\n\n-- Step 2: Add to checking\nUPDATE Account\nSET Balance = Balance + 500\nWHERE AccountID = 101 AND AccountType = 'Checking';\n\n-- Verify balance constraints\nIF (SELECT Balance FROM Account WHERE AccountID=101 AND AccountType='Savings') < 0\n  ROLLBACK;\nELSE\n  COMMIT;\nEND TRANSACTION;\n\nExample 2: Deadlock Scenario\n\nTime | Transaction T1              | Transaction T2\n-----|----------------------------|---------------------------\nt1   | BEGIN                      |\nt2   |                            | BEGIN\nt3   | LOCK A (Exclusive)         |\nt4   |                            | LOCK B (Exclusive)\nt5   | UPDATE A SET ...           |\nt6   |                            | UPDATE B SET ...\nt7   | LOCK B (Wait for T2)       |\nt8   |                            | LOCK A (Wait for T1)\nt9   | DEADLOCK DETECTED          |\nt10  | T2 ABORTED (Victim)        |\nt11  | LOCK B (Acquired)          |\nt12  | COMMIT                     |\n\nExample 3: Recovery Scenario\n\nLog Contents:\n<T1 start>\n<T1, X, 100, 150>\n<T1, Y, 200, 250>\n<T1 commit>\n<T2 start>\n<T2, X, 150, 200>\n<T2, Y, 250, 300>\n--- CRASH ---\n\nRecovery:\n1. Redo T1 (committed): X=150, Y=250\n2. Undo T2 (not committed): X=150 (revert T2's change), Y=250\n\nFinal State: X=150, Y=250\n\nExample 4: Isolation Levels in Action\n\n-- Session 1\nSET TRANSACTION ISOLATION LEVEL READ UNCOMMITTED;\nBEGIN TRANSACTION;\nSELECT Balance FROM Account WHERE ID=101; -- Reads 1000\n-- (Session 2 updates Balance to 500 but doesn't commit)\nSELECT Balance FROM Account WHERE ID=101; -- Reads 500 (DIRTY READ!)\nCOMMIT;\n\n-- With READ COMMITTED\nSET TRANSACTION ISOLATION LEVEL READ COMMITTED;\nBEGIN TRANSACTION;\nSELECT Balance FROM Account WHERE ID=101; -- Reads 1000\n-- (Session 2 updates to 500 but doesn't commit)\nSELECT Balance FROM Account WHERE ID=101; -- Still reads 1000\nCOMMIT;\n\n-- With SERIALIZABLE (PostgreSQL)\nSET TRANSACTION ISOLATION LEVEL SERIALIZABLE;\nBEGIN TRANSACTION;\nSELECT SUM(Balance) FROM Account; -- Reads sum\n-- (Session 2 inserts new account)\nSELECT SUM(Balance) FROM Account; -- Same sum (no phantom)\nCOMMIT;",
          practical: "Best Practices:\n\n1. Transaction Design:\n   - Keep transactions short\n   - Acquire locks in consistent order (prevent deadlock)\n   - Avoid user interaction within transaction\n   - Handle exceptions (rollback on error)\n\n2. Choosing Isolation Level:\n   - READ UNCOMMITTED: Never (except analytics)\n   - READ COMMITTED: Default for most applications\n   - REPEATABLE READ: Financial transactions, reports\n   - SERIALIZABLE: Critical data, when correctness > performance\n\n3. Performance Tuning:\n   - Monitor lock wait times\n   - Identify long-running transactions\n   - Use row-level locking (not table-level)\n   - Consider optimistic locking for low contention\n\n4. Deadlock Prevention:\n   - Access tables in same order\n   - Use timeouts\n   - Keep transactions short\n   - Retry aborted transactions\n\n5. Application-Level Considerations:\n   - Implement retry logic\n   - Use connection pooling\n   - Consider eventual consistency (NoSQL)\n   - Saga pattern for distributed transactions\n\nReal-World Usage:\n- Banking: SERIALIZABLE for transfers\n- E-commerce: READ COMMITTED for order processing\n- Social media: READ COMMITTED, eventual consistency for likes\n- Stock trading: SERIALIZABLE with strict locking\n- Analytics: READ UNCOMMITTED (stale data acceptable)",
          exam: "Key Questions:\n\n1. Explain ACID properties with examples\n2. Draw transaction state diagram\n3. What problems arise from concurrent transactions? Give examples.\n4. Explain Two-Phase Locking protocol. How does it ensure serializability?\n5. Compare timestamp-based vs lock-based concurrency control\n6. What is a deadlock? Explain detection and prevention strategies.\n7. Explain Write-Ahead Logging and its importance in recovery\n8. Describe ARIES recovery algorithm (Analysis, Redo, Undo)\n9. Compare SQL isolation levels. Which allows which anomalies?\n10. Given a schedule, determine if it's serializable\n11. Show recovery steps given transaction log and crash point\n12. Explain MVCC. Why do readers not block writers?",
          takeaways: "Core Concepts:\n\n• ACID properties guarantee reliable transactions\n• Atomicity: All or nothing (enforced by logging and recovery)\n• Consistency: Maintain constraints (enforced by application + DBMS)\n• Isolation: Concurrent transactions don't interfere (concurrency control)\n• Durability: Committed changes survive crashes (write-ahead logging)\n• Two-Phase Locking ensures serializability but can deadlock\n• MVCC allows high concurrency by keeping multiple versions\n• Recovery uses logs to redo committed and undo uncommitted transactions\n• Isolation levels balance consistency with performance"
        }
      }
    ]
  },
  {
    id: "unit-5",
    title: "UNIT V: Security & Advanced Topics",
    topics: [
      {
        id: "database-security",
        title: "Database Security",
        subtopics: ["Authentication", "Authorization and access control", "DAC", "MAC and RBAC models", "Intrusion detection SQL injection"],
        clos: ["CLO01"],
        cos: ["CO06"],
        content: {
          introduction: "In 2023, data breaches exposed over 3 billion records globally. Equifax breach, Capital One breach, Marriott breach—the list goes on. Database security isn't optional; it's essential. A single SQL injection or weak access control can expose millions of customer records, leading to financial losses, legal consequences, and destroyed trust.",
          concept: "Database security protects against unauthorized access, data breaches, and malicious attacks through multiple layers of defense.\n\nSecurity Objectives (CIA Triad):\n\n1. Confidentiality: Only authorized users access data\n   Prevent: Unauthorized disclosure, eavesdropping\n\n2. Integrity: Data accuracy and consistency\n   Prevent: Unauthorized modification, corruption\n\n3. Availability: Authorized users can access when needed\n   Prevent: DoS attacks, system failures\n\nSecurity Layers:\n\n1. Network Security: Firewalls, VPNs, encryption\n2. OS Security: User accounts, file permissions\n3. Database Security: Authentication, authorization, auditing\n4. Application Security: Input validation, secure coding\n\nKey Threats:\n- SQL Injection: Malicious SQL in user input\n- Unauthorized Access: Weak passwords, stolen credentials\n- Privilege Escalation: Gaining higher access than authorized\n- Insider Threats: Malicious employees\n- Data Leakage: Unencrypted data transmission\n- DoS/DDoS: Overwhelming system with requests",
          technicalDepth: "AUTHENTICATION:\n\nVerifying user identity\n\nMethods:\n\n1. Password-Based:\n   - Most common\n   - Store hashed passwords (SHA-256, bcrypt)\n   - Never store plaintext\n   - Salting prevents rainbow table attacks\n\n2. Multi-Factor Authentication (MFA):\n   - Something you know (password)\n   - Something you have (token, phone)\n   - Something you are (biometric)\n\n3. Certificate-Based:\n   - Digital certificates (X.509)\n   - Public key infrastructure (PKI)\n\n4. OAuth/SAML:\n   - Federated authentication\n   - Single Sign-On (SSO)\n\nAUTHORIZATION & ACCESS CONTROL:\n\nDetermining what authenticated users can do\n\n1. DISCRETIONARY ACCESS CONTROL (DAC):\n\nOwner controls access to resources\n\nSQL Commands:\nGRANT SELECT, INSERT ON Employees TO user1;\nGRANT ALL PRIVILEGES ON DATABASE company TO admin;\nREVOKE DELETE ON Orders FROM user2;\n\nGRANT Options:\n- SELECT: Read data\n- INSERT: Add new data\n- UPDATE: Modify existing data\n- DELETE: Remove data\n- EXECUTE: Run stored procedures\n\nWITH GRANT OPTION: User can grant permissions to others\nGRANT SELECT ON Employees TO user1 WITH GRANT OPTION;\n\nProblems with DAC:\n- Permissions can spread uncontrollably\n- No central policy enforcement\n- Vulnerable to Trojan horse attacks\n\n2. MANDATORY ACCESS CONTROL (MAC):\n\nSystem enforces access policy based on security labels\n\nConcepts:\n- Security Clearance: User's level (Top Secret, Secret, Unclassified)\n- Security Classification: Data's level\n- Bell-LaPadula Model: \n  - No Read Up: Can't read higher classification\n  - No Write Down: Can't write to lower classification\n  - Prevents information leakage\n\nExample:\nUser with SECRET clearance:\n- Can read: UNCLASSIFIED, SECRET\n- Cannot read: TOP SECRET\n- Can write: SECRET, TOP SECRET\n- Cannot write: UNCLASSIFIED (prevents leaking)\n\nUsed in: Military, government systems\n\n3. ROLE-BASED ACCESS CONTROL (RBAC):\n\n![RBAC Model](/images/unit5/rbac-model.svg)\n\nPermissions assigned to roles, users assigned to roles\n\nAdvantages:\n- Easier management (modify role, not individual users)\n- Principle of least privilege\n- Separation of duties\n\nSQL Implementation:\n\nCREATE ROLE hr_manager;\nGRANT SELECT, UPDATE ON Employees TO hr_manager;\nGRANT SELECT ON Salaries TO hr_manager;\n\nCREATE ROLE accountant;\nGRANT SELECT ON Salaries TO accountant;\nGRANT INSERT, UPDATE ON Expenses TO accountant;\n\nGRANT hr_manager TO alice;\nGRANT accountant TO bob;\n\nRole Hierarchies:\nCREATE ROLE manager;\nGRANT hr_manager TO manager; -- Manager inherits HR permissions\n\nSQL INJECTION:\n\nAttack Mechanism:\nUser input interpreted as SQL code\n\nVulnerable Code (Python):\nusername = request.form['username']\npassword = request.form['password']\nquery = f\"SELECT * FROM Users WHERE username='{username}' AND password='{password}'\"\nexecute(query)\n\nAttack Input:\nusername: admin' --\npassword: anything\n\nResulting Query:\nSELECT * FROM Users WHERE username='admin' -- ' AND password='anything'\n(-- comments out rest, bypasses password check)\n\nWorse Attack (Union-based):\nusername: ' UNION SELECT * FROM CreditCards --\n\nResulting Query:\nSELECT * FROM Users WHERE username='' UNION SELECT * FROM CreditCards -- '\n(Returns credit card data!)\n\nPrevention:\n\n1. Parameterized Queries (Prepared Statements):\n\nSecure Code (Python):\nquery = \"SELECT * FROM Users WHERE username=? AND password=?\"\nexecute(query, (username, password))\n\nDatabase treats ? as data, not code\n\n2. Stored Procedures:\nCREATE PROCEDURE AuthenticateUser(@username NVARCHAR(50), @password NVARCHAR(50))\nAS\nBEGIN\n  SELECT * FROM Users WHERE username=@username AND password=@password;\nEND;\n\nCALL AuthenticateUser('admin', 'pass123');\n\n3. Input Validation:\n- Whitelist allowed characters\n- Reject suspicious input ('; --, UNION, etc.)\n- Escape special characters\n\n4. Least Privilege:\n- Application database account has minimal permissions\n- No DROP, CREATE permissions\n\n5. Web Application Firewall (WAF):\n- Detects and blocks SQL injection patterns\n\nENCRYPTION:\n\n1. Data at Rest:\n   - Encrypt database files\n   - Transparent Data Encryption (TDE)\n   - Column-level encryption for sensitive fields\n\n2. Data in Transit:\n   - SSL/TLS for connections\n   - HTTPS for web applications\n\n3. Key Management:\n   - Secure key storage (HSM, Key Vault)\n   - Key rotation policies\n\nAUDITING:\n\nTrack who accessed what, when\n\nAudit Log Contents:\n- User ID\n- Timestamp\n- Action (SELECT, INSERT, UPDATE, DELETE)\n- Table/row accessed\n- Success/failure\n- Source IP\n\nSQL Server Audit:\nCREATE SERVER AUDIT CompanyAudit\nTO FILE (FILEPATH = 'C:\\Audits\\')\nWITH (ON_FAILURE = CONTINUE);\n\nCREATE DATABASE AUDIT SPECIFICATION SalaryAudit\nFOR SERVER AUDIT CompanyAudit\nADD (SELECT, UPDATE ON Salaries BY public);\n\nBenefits:\n- Detect unauthorized access\n- Forensics after breach\n- Compliance (GDPR, HIPAA, SOX)",
          examples: "Example 1: SQL Injection Attack & Prevention\n\nVulnerable Login (PHP):\n$username = $_POST['username'];\n$password = $_POST['password'];\n$query = \"SELECT * FROM users WHERE username='$username' AND password='$password'\";\n$result = mysqli_query($conn, $query);\n\nAttack:\nUsername: admin' OR '1'='1\nPassword: anything\n\nQuery becomes:\nSELECT * FROM users WHERE username='admin' OR '1'='1' AND password='anything'\n(Always true, logs in as admin!)\n\nSecure Version (Prepared Statement):\n$stmt = $conn->prepare(\"SELECT * FROM users WHERE username=? AND password=?\");\n$stmt->bind_param(\"ss\", $username, $password);\n$stmt->execute();\n$result = $stmt->get_result();\n\nExample 2: RBAC Implementation\n\n-- Create roles\nCREATE ROLE sales_rep;\nCREATE ROLE sales_manager;\nCREATE ROLE admin;\n\n-- Grant permissions to roles\nGRANT SELECT ON Customers TO sales_rep;\nGRANT SELECT, INSERT ON Orders TO sales_rep;\n\nGRANT SELECT ON Customers TO sales_manager;\nGRANT SELECT, INSERT, UPDATE ON Orders TO sales_manager;\nGRANT SELECT ON SalesReport TO sales_manager;\n\nGRANT ALL PRIVILEGES ON DATABASE company TO admin;\n\n-- Assign users to roles\nGRANT sales_rep TO john, mary;\nGRANT sales_manager TO alice;\nGRANT admin TO bob;\n\n-- Role hierarchy (manager inherits rep permissions)\nGRANT sales_rep TO sales_manager;\n\nExample 3: Encryption\n\n-- Column-level encryption (SQL Server)\nCREATE MASTER KEY ENCRYPTION BY PASSWORD = 'StrongPassword123!';\n\nCREATE CERTIFICATE SSNCert\nWITH SUBJECT = 'SSN Encryption Certificate';\n\nCREATE SYMMETRIC KEY SSNKey\nWITH ALGORITHM = AES_256\nENCRYPTION BY CERTIFICATE SSNCert;\n\n-- Encrypt data\nOPEN SYMMETRIC KEY SSNKey\nDECRYPTION BY CERTIFICATE SSNCert;\n\nINSERT INTO Employees (Name, SSN_Encrypted)\nVALUES ('John Doe', EncryptByKey(Key_GUID('SSNKey'), '123-45-6789'));\n\nCLOSE SYMMETRIC KEY SSNKey;\n\n-- Decrypt data\nOPEN SYMMETRIC KEY SSNKey\nDECRYPTION BY CERTIFICATE SSNCert;\n\nSELECT Name, CONVERT(VARCHAR, DecryptByKey(SSN_Encrypted)) AS SSN\nFROM Employees;\n\nCLOSE SYMMETRIC KEY SSNKey;\n\nExample 4: Auditing\n\n-- Enable auditing for sensitive table (PostgreSQL)\nCREATE TABLE audit_log (\n    audit_id SERIAL PRIMARY KEY,\n    table_name VARCHAR(50),\n    action VARCHAR(10),\n    user_name VARCHAR(50),\n    timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n    old_data JSON,\n    new_data JSON\n);\n\n-- Trigger to log changes\nCREATE OR REPLACE FUNCTION log_salary_changes()\nRETURNS TRIGGER AS $$\nBEGIN\n    IF TG_OP = 'UPDATE' THEN\n        INSERT INTO audit_log (table_name, action, user_name, old_data, new_data)\n        VALUES ('Salaries', 'UPDATE', current_user, \n                row_to_json(OLD), row_to_json(NEW));\n    END IF;\n    RETURN NEW;\nEND;\n$$ LANGUAGE plpgsql;\n\nCREATE TRIGGER salary_audit\nAFTER UPDATE ON Salaries\nFOR EACH ROW EXECUTE FUNCTION log_salary_changes();",
          practical: "Best Practices:\n\n1. Defense in Depth:\n   - Multiple security layers\n   - Firewall + Authentication + Authorization + Encryption + Auditing\n   - If one fails, others still protect\n\n2. Principle of Least Privilege:\n   - Grant minimum necessary permissions\n   - Application account: Only needed tables\n   - Read-only users: No write permissions\n\n3. Input Validation:\n   - Validate all user input\n   - Use parameterized queries\n   - Whitelist, not blacklist\n\n4. Regular Security Audits:\n   - Review user permissions\n   - Check audit logs for anomalies\n   - Penetration testing\n\n5. Encryption:\n   - Encrypt sensitive data at rest\n   - Use SSL/TLS for connections\n   - Secure key management\n\n6. Patch Management:\n   - Keep database software updated\n   - Apply security patches promptly\n\n7. Monitoring & Alerting:\n   - Monitor failed login attempts\n   - Alert on unusual access patterns\n   - Real-time intrusion detection\n\nCompliance Standards:\n- GDPR: EU data protection\n- HIPAA: Healthcare data (US)\n- PCI DSS: Credit card data\n- SOX: Financial reporting\n\nReal-World Scenarios:\n- Healthcare: Encrypt patient records, audit all access\n- Banking: MFA, transaction auditing, encryption\n- E-commerce: PCI DSS compliance, secure payment data\n- Government: MAC for classified data",
          exam: "Important Questions:\n\n1. Explain the CIA triad in database security\n2. Compare DAC, MAC, and RBAC with examples\n3. What is SQL injection? How to prevent it?\n4. Show SQL injection attack and secure code\n5. Explain role-based access control with SQL examples\n6. What is the difference between authentication and authorization?\n7. Why use parameterized queries instead of string concatenation?\n8. Explain encryption at rest vs encryption in transit\n9. What is database auditing? Why is it important?\n10. Explain multi-factor authentication\n11. What is the principle of least privilege?\n12. How does GRANT and REVOKE work in SQL?",
          takeaways: "Key Principles:\n\n• Security requires multiple layers: authentication, authorization, encryption, auditing\n• SQL injection is preventable: use parameterized queries, never concatenate SQL\n• RBAC simplifies permission management: assign permissions to roles, not individuals\n• Principle of least privilege: grant minimum necessary permissions\n• Encryption protects data at rest and in transit\n• Auditing provides accountability and forensics\n• Defense in depth: multiple security layers provide redundancy\n• Security is ongoing: monitor, patch, audit regularly"
        }
      },
      {
        id: "advanced-topics",
        title: "Advanced Topics",
        subtopics: ["Object oriented and object relational databases", "Logical warehousing and data mining"],
        clos: ["CLO01"],
        cos: ["CO06"],
        content: {
          introduction: "Relational databases dominated for decades, but new challenges emerged: How to store complex objects? How to analyze petabytes of data? How to handle unstructured data? Advanced database technologies—object-oriented databases, data warehousing, NoSQL, and distributed systems—evolved to address these needs.",
          concept: "Advanced database topics extend beyond traditional relational databases to handle complex data types, massive-scale analytics, and modern application requirements.\n\nKey Motivations:\n\n1. Complex Data: Multimedia, CAD, scientific simulations\n2. Big Data: Petabytes of data, distributed processing\n3. Performance: Real-time analytics, low-latency access\n4. Scalability: Horizontal scaling, cloud-native\n5. Flexibility: Schema evolution, heterogeneous data",
          technicalDepth: "OBJECT-ORIENTED DATABASES (OODBMS):\n\nMotivation: Impedance mismatch between OOP languages and relational databases\n\nRelational Approach:\nclass Employee {\n    int id;\n    String name;\n    Address address; // Complex object\n}\n\n// Must decompose into multiple tables\nEmployees(id, name, address_id)\nAddresses(address_id, street, city, zip)\n\n// Retrieve requires joins, complex mapping\n\nOODBMS Approach:\nStore objects directly, no decomposition\nQuery using object-oriented syntax\n\nFeatures:\n\n1. Complex Objects:\n   - Nested objects\n   - Collections (lists, sets)\n   - References between objects\n\n2. Object Identity (OID):\n   - Each object has unique identifier\n   - Independent of attribute values\n   - Persists across sessions\n\n3. Encapsulation:\n   - Methods stored with data\n   - Behavior + data together\n\n4. Inheritance:\n   - Class hierarchies\n   - Subclasses inherit from superclasses\n\n5. Polymorphism:\n   - Method overriding\n   - Dynamic binding\n\nOODBMS Examples:\n- db4o (Java, .NET)\n- ObjectDB (Java)\n- Versant\n\nObject-Relational Databases (ORDBMS):\n\nHybrid: Relational model + OO features\n\nPostgreSQL Example:\n-- User-defined types\nCREATE TYPE Address AS (\n    street VARCHAR(100),\n    city VARCHAR(50),\n    zip VARCHAR(10)\n);\n\nCREATE TABLE Employee (\n    id INT PRIMARY KEY,\n    name VARCHAR(100),\n    address Address  -- Complex type\n);\n\nINSERT INTO Employee VALUES (1, 'John', ROW('123 Main St', 'NYC', '10001'));\n\nSELECT name, (address).city FROM Employee;\n\nAdvantages:\n- Natural mapping from OOP\n- No joins for complex objects\n- Better performance for complex data\n\nDisadvantages:\n- Less mature than RDBMS\n- Smaller ecosystem\n- No standard query language (unlike SQL)\n\nDATA WAREHOUSING:\n\nDefinition: Subject-oriented, integrated, time-variant, non-volatile collection for decision support\n\nCharacteristics:\n\n1. Subject-Oriented: Organized by business area (Sales, Finance)\n2. Integrated: Data from multiple sources\n3. Time-Variant: Historical data, snapshots\n4. Non-Volatile: Read-only, no updates\n\nArchitecture:\n\nSource Systems (OLTP)\n    ↓\nETL (Extract, Transform, Load)\n    ↓\nData Warehouse\n    ↓\nData Marts (department-specific)\n    ↓\nOLAP / BI Tools\n\nOLTP vs OLAP:\n\nOLTP (Online Transaction Processing):\n- Current data\n- Detailed records\n- Read/Write\n- Normalized (3NF)\n- Fast transactions\n- Example: Bank account balance\n\nOLAP (Online Analytical Processing):\n- Historical data\n- Aggregated summaries\n- Read-mostly\n- Denormalized (star schema)\n- Complex queries\n- Example: Quarterly sales analysis\n\nDimensional Modeling:\n\nStar Schema:\n\n        Dimension: Time\n             |\n             |\nDimension -- Fact Table -- Dimension\n (Product)   (Sales)      (Customer)\n             |\n             |\n        Dimension: Store\n\nFact Table: Measures (sales amount, quantity)\nDimension Tables: Context (who, what, when, where)\n\nSnowflake Schema:\nNormalized dimensions (dimension tables have sub-dimensions)\n\nOLAP Operations:\n\n1. Roll-Up: Aggregate to higher level\n   Daily sales → Monthly sales\n\n2. Drill-Down: Detail to lower level\n   Yearly sales → Quarterly sales\n\n3. Slice: Fix one dimension\n   Sales in Q1 2024\n\n4. Dice: Fix multiple dimensions\n   Sales in Q1 2024, Region=West, Product=Laptop\n\n5. Pivot: Rotate view\n   Rows ↔ Columns\n\nDATA MINING:\n\nDefinition: Discovering patterns, correlations, trends in large datasets\n\nKnowledge Discovery Process (KDD):\n\n1. Data Cleaning: Remove noise, outliers\n2. Data Integration: Combine sources\n3. Data Selection: Choose relevant data\n4. Data Transformation: Normalize, aggregate\n5. Data Mining: Apply algorithms\n6. Pattern Evaluation: Identify interesting patterns\n7. Knowledge Presentation: Visualize, report\n\nData Mining Techniques:\n\n1. Association Rules:\n   Market Basket Analysis\n   {Bread, Butter} ⇒ {Milk} (support=30%, confidence=80%)\n   Support: % transactions containing itemset\n   Confidence: % transactions with antecedent that also have consequent\n\n2. Classification:\n   Assign items to predefined categories\n   Decision Trees, Neural Networks, SVM\n   Example: Spam vs Not Spam email\n\n3. Clustering:\n   Group similar items (no predefined categories)\n   K-Means, Hierarchical Clustering\n   Example: Customer segmentation\n\n4. Regression:\n   Predict continuous values\n   Linear Regression, Polynomial Regression\n   Example: Predict house price\n\n5. Anomaly Detection:\n   Identify outliers\n   Example: Fraud detection\n\n6. Sequential Patterns:\n   Find patterns over time\n   Example: Web clickstream analysis\n\nDISTRIBUTED DATABASES:\n\nData stored across multiple locations\n\nTypes:\n\n1. Homogeneous: Same DBMS at all sites\n2. Heterogeneous: Different DBMS at different sites\n\nFragmentation:\n\n1. Horizontal: Split rows\n   West_Customers (CA, WA, OR)\n   East_Customers (NY, MA, PA)\n\n2. Vertical: Split columns\n   Employee_Basic (id, name, dept)\n   Employee_Payroll (id, salary, bonus)\n\n3. Hybrid: Both\n\nReplication:\n- Full: Complete copy at each site\n- Partial: Selected fragments at sites\n\nAdvantages:\n- Improved performance (data locality)\n- Increased availability (redundancy)\n- Scalability\n\nChallenges:\n- Distributed query processing\n- Distributed transactions (2PC)\n- Consistency vs Availability (CAP theorem)\n\nNOSQL DATABASES:\n\nMotivation: Scalability, flexibility, performance for web-scale applications\n\nTypes:\n\n1. Key-Value: Redis, DynamoDB\n   Simple: key → value\n   Fast lookups\n   Use case: Session storage, caching\n\n2. Document: MongoDB, CouchDB\n   Store JSON/BSON documents\n   Schema-less\n   Use case: Content management, catalogs\n\n3. Column-Family: Cassandra, HBase\n   Wide columns, billions of columns\n   Distributed, high write throughput\n   Use case: Time-series, logs\n\n4. Graph: Neo4j, Amazon Neptune\n   Nodes and edges\n   Relationships are first-class\n   Use case: Social networks, recommendations\n\nCAP Theorem:\nCan have at most 2 of 3:\n- Consistency: All nodes see same data\n- Availability: Every request gets response\n- Partition Tolerance: Works despite network splits\n\nRelational: CA (sacrifice partition tolerance)\nNoSQL: Often AP or CP",
          examples: "Example 1: OODBMS (db4o - Java)\n\n// Define classes\nclass Employee {\n    String name;\n    Address address;\n    List<Project> projects;\n}\n\nclass Address {\n    String street;\n    String city;\n}\n\n// Store object directly\nEmployee emp = new Employee();\nemp.name = \"John\";\nemp.address = new Address(\"123 Main\", \"NYC\");\ndb.store(emp); // Stores entire object graph\n\n// Query\nList<Employee> result = db.query(new Predicate<Employee>() {\n    public boolean match(Employee emp) {\n        return emp.address.city.equals(\"NYC\");\n    }\n});\n\nExample 2: Data Warehouse Star Schema (SQL)\n\nCREATE TABLE FactSales (\n    sale_id INT PRIMARY KEY,\n    date_id INT,\n    product_id INT,\n    customer_id INT,\n    store_id INT,\n    quantity INT,\n    amount DECIMAL(10,2),\n    FOREIGN KEY (date_id) REFERENCES DimDate(date_id),\n    FOREIGN KEY (product_id) REFERENCES DimProduct(product_id),\n    FOREIGN KEY (customer_id) REFERENCES DimCustomer(customer_id),\n    FOREIGN KEY (store_id) REFERENCES DimStore(store_id)\n);\n\nCREATE TABLE DimDate (\n    date_id INT PRIMARY KEY,\n    date DATE,\n    day INT,\n    month INT,\n    quarter INT,\n    year INT\n);\n\nCREATE TABLE DimProduct (\n    product_id INT PRIMARY KEY,\n    product_name VARCHAR(100),\n    category VARCHAR(50),\n    brand VARCHAR(50)\n);\n\n-- Analytical Query: Quarterly sales by product category\nSELECT d.quarter, p.category, SUM(f.amount) AS total_sales\nFROM FactSales f\nJOIN DimDate d ON f.date_id = d.date_id\nJOIN DimProduct p ON f.product_id = p.product_id\nWHERE d.year = 2024\nGROUP BY d.quarter, p.category\nORDER BY d.quarter, total_sales DESC;\n\nExample 3: NoSQL (MongoDB)\n\n// Document database - store complex objects\ndb.employees.insertOne({\n    _id: 1,\n    name: \"John Doe\",\n    address: {\n        street: \"123 Main St\",\n        city: \"NYC\",\n        zip: \"10001\"\n    },\n    projects: [\n        { name: \"Project A\", role: \"Lead\" },\n        { name: \"Project B\", role: \"Developer\" }\n    ],\n    skills: [\"Java\", \"Python\", \"SQL\"]\n});\n\n// Query nested fields\ndb.employees.find({ \"address.city\": \"NYC\" });\n\n// Query arrays\ndb.employees.find({ skills: \"Python\" });\n\nExample 4: Data Mining - Association Rules (Python)\n\nfrom mlxtend.frequent_patterns import apriori, association_rules\nimport pandas as pd\n\n# Transaction data\ntransactions = [\n    ['Bread', 'Milk', 'Butter'],\n    ['Bread', 'Butter'],\n    ['Milk', 'Butter', 'Eggs'],\n    ['Bread', 'Milk', 'Butter', 'Eggs'],\n    ['Bread', 'Milk']\n]\n\n# Find frequent itemsets\nfrequent_itemsets = apriori(df, min_support=0.5, use_colnames=True)\n\n# Generate association rules\nrules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.7)\nprint(rules[['antecedents', 'consequents', 'support', 'confidence']])\n\n# Output: {Bread, Butter} => {Milk} (confidence=0.8)",
          practical: "When to Use Each Technology:\n\n1. Relational Databases (PostgreSQL, MySQL):\n   - Structured data\n   - ACID transactions required\n   - Complex queries, joins\n   - Examples: Banking, e-commerce orders\n\n2. OODBMS / ORDBMS:\n   - Complex nested objects\n   - Multimedia, CAD, scientific data\n   - Tight OOP integration\n   - Examples: GIS, engineering simulations\n\n3. Data Warehouses:\n   - Historical analysis\n   - Business intelligence\n   - Reporting, dashboards\n   - Examples: Sales analytics, financial reporting\n\n4. NoSQL:\n   Document (MongoDB): Flexible schema, content management\n   Key-Value (Redis): Caching, session storage\n   Column-Family (Cassandra): Time-series, logs, high write throughput\n   Graph (Neo4j): Social networks, recommendations, fraud detection\n\n5. Distributed Databases:\n   - Global applications\n   - High availability requirements\n   - Scale beyond single server\n   - Examples: Multi-region applications\n\nModern Architecture Trends:\n- Polyglot Persistence: Use right database for each use case\n- Lambda Architecture: Batch + Stream processing\n- Data Lakes: Store raw data, process on-demand\n- Cloud Data Warehouses: Snowflake, BigQuery, Redshift",
          exam: "Key Questions:\n\n1. Compare relational databases with object-oriented databases\n2. What is impedance mismatch? How does OODBMS solve it?\n3. Explain star schema and snowflake schema\n4. What is the difference between OLTP and OLAP?\n5. Describe the ETL process in data warehousing\n6. Explain OLAP operations: roll-up, drill-down, slice, dice, pivot\n7. What is data mining? List and explain 3 techniques\n8. Explain association rule mining with example\n9. What is the CAP theorem? How does it relate to NoSQL?\n10. Compare document, key-value, column-family, and graph databases\n11. What is horizontal vs vertical fragmentation in distributed databases?\n12. When would you choose NoSQL over relational database?",
          takeaways: "Core Concepts:\n\n• OODBMS stores objects directly, eliminating impedance mismatch\n• Data warehouses support analytics with denormalized star/snowflake schemas\n• OLTP focuses on transactions, OLAP on analysis\n• Data mining discovers patterns: association rules, classification, clustering\n• NoSQL provides scalability and flexibility for specific use cases\n• CAP theorem: Can't have consistency, availability, and partition tolerance together\n• Distributed databases provide scalability through fragmentation and replication\n• Modern applications use polyglot persistence: right database for each use case"
        }
      }
    ]
  }
];
